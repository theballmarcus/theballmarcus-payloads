#!/usr/bin/env python3
"""
Fuzz Helper Tool

Converts a HTTP(S) request file into fuzzing requests.
and optionally performs character-by-character fuzzing.
"""
import argparse
from ast import Str
import json
import os
import string
import sys
from collections import Counter
from typing import Any, Dict, List, Tuple, Union, Generator, Optional
from unittest import result
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
import statistics
import importlib.util
import re

VERBOSE = False
MAX_WORKERS = 10  # Default max workers for ThreadPoolExecutor
DEFAULT_WORDLISTS = {
    'rockyou' : '~/wordlist/rockyou.txt',
    'directories' : '~/wordlist/directory-list-2.3-small.txt',
    'usernames' : '~/wordlist/usernames.txt',
}
SAFE_CHARS = set(string.ascii_letters + string.digits + '-_')

## ----- Response Analysis ----- ##

def find_most_probable_character(data: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
    """
    Identify deviations from the majority response pattern to find probable characters.
    Returns dict with 'safe_chars' and 'special_chars'.
    If no deviations are found, but only one safe_char appears in error responses, assume it's the probable character.
    """
    pattern_counts = Counter((e['chars'], e['words'], e['lines']) for e in data)
    if not pattern_counts:
        return {'safe_chars': [], 'special_chars': []}

    majority_pattern, _ = pattern_counts.most_common(1)[0]
    deviations = []

    for e in data:
        if e.get('code', 0) < 400:
            patt = (e['chars'], e['words'], e['lines'])
            if patt != majority_pattern:
                dev = sum(abs(a - b) for a, b in zip(patt, majority_pattern))
                deviations.append((dev, e))

    deviations.sort(key=lambda x: x[0], reverse=True)
    result = {'safe_chars': [], 'special_chars': []}

    if deviations:
        max_dev = deviations[0][0]
        for dev, e in deviations:
            if dev != max_dev:
                break
            char = e.get('payload', '')
            details = {'chars': e['chars'], 'words': e['words'], 'lines': e['lines'], 'deviation': dev}
            key = 'safe_chars' if len(char) == 1 and char in SAFE_CHARS else 'special_chars'
            result[key].append({'char': char, 'details': details})

    # Fallback: If no deviations matched, check for single unique safe_char among errors
    if not result['safe_chars'] and not result['special_chars']:
        error_safe_chars = {
            e.get('payload', ''): e for e in data
            if e.get('code', 0) >= 400 and len(e.get('payload', '')) == 1 and e.get('payload') in SAFE_CHARS
        }
        if len(error_safe_chars) == 1:
            char, e = next(iter(error_safe_chars.items()))
            details = {'chars': e['chars'], 'words': e['words'], 'lines': e['lines'], 'deviation': None}
            result['safe_chars'].append({'char': char, 'details': details})

    return result

def find_most_probable_time_based_character(
    data: List[Dict[str, Any]],
    deviation_threshold: float = 1  # 35% by default
) -> Union[Dict[str, Any], bool]:
    """
    Identify the character with the largest positive deviation in response time from the mean.
    Return False if no character deviates enough (based on deviation_threshold).
    
    Parameters:
        - data: list of request/response records
        - deviation_threshold: minimum percent deviation from mean to be considered significant

    Returns:
        - dict with 'most_probable', 'deviation', and 'all_char_stats', or
        - False if no character significantly deviates
    """
    if not data:
        return False

    # Group by payloads
    payload_groups = {}
    for e in data:
        payload = e.get('payload', '')
        if len(payload) != 1:
            continue  # Skip multi-char payloads
        payload_groups.setdefault(payload, []).append(e)

    char_stats = []
    
    for payload, group in payload_groups.items():
        avg_response_time = sum(e['response_time'] for e in group) / len(group)
        stats = {
            'char': payload,
            'response_time': avg_response_time,
            'count': len(group),
            'details': {
                'chars': sum(e['chars'] for e in group),
                'words': sum(e['words'] for e in group),
                'lines': sum(e['lines'] for e in group),
            }
        }
        char_stats.append(stats)

    if not char_stats:
        return False

    # Compute global average
    global_mean = statistics.mean(c['response_time'] for c in char_stats)

    # Add deviation to each
    for c in char_stats:
        c['deviation'] = c['response_time'] - global_mean
        c['relative_deviation'] = c['deviation'] / global_mean if global_mean else 0

    # Get the max deviation entry
    most_probable = max(char_stats, key=lambda x: x['deviation'])
    print(f"Most probable character: {most_probable['char']} with deviation {most_probable['deviation']} and relative deviation {most_probable['relative_deviation']:.2%}")
    # Check if the deviation is significant enough
    if most_probable['relative_deviation'] < deviation_threshold:
        return False

    return {
        'most_probable': most_probable['char'],
        'deviation': most_probable['deviation'],
        'relative_deviation': most_probable['relative_deviation'],
        'all_char_stats': char_stats
    }

# def analyze_responses(responses: List[Dict[str, Any]], time_based: bool) -> Optional[str]:
#     """
#     Analyze responses to find the most probable character.
#     Uses different strategies based on attack type.
#     """
#     if not responses:
#         return None
    
#     if time_based:
#         # Group by payload characters
#         char_groups = {}
#         for r in responses:
#             char = r.get('candidate_char', '')
#             if char:
#                 char_groups.setdefault(char, []).append(r)
        
#         # Calculate average response times
#         char_stats = []
#         for char, group in char_groups.items():
#             avg_time = sum(r['response_time'] for r in group) / len(group)
#             char_stats.append({'char': char, 'avg_time': avg_time})
        
#         if not char_stats:
#             return None
        
#         # Find global average
#         global_avg = statistics.mean(s['avg_time'] for s in char_stats)
        
#         # Find character with max deviation
#         max_deviation = 0
#         probable_char = None
#         for stat in char_stats:
#             deviation = stat['avg_time'] - global_avg
#             if deviation > max_deviation:
#                 max_deviation = deviation
#                 probable_char = stat['char']
                
#         # Check if deviation is significant (35% threshold)
#         if max_deviation > global_avg * 0.35:
#             return probable_char
#         return None
    
#     else:
#         # Non-time-based analysis
#         pattern_counts = Counter(
#             (e['chars'], e['words'], e['lines']) for e in responses
#         )
#         if not pattern_counts:
#             return None

#         majority_pattern, _ = pattern_counts.most_common(1)[0]
#         deviations = []

#         for e in responses:
#             if e.get('code', 0) < 400:
#                 pattern = (e['chars'], e['words'], e['lines'])
#                 if pattern != majority_pattern:
#                     dev = sum(abs(a - b) for a, b in zip(pattern, majority_pattern))
#                     deviations.append((dev, e))

#         if not deviations:
#             # Fallback: look for single safe char in error responses
#             error_chars = {
#                 e.get('candidate_char', ''): e for e in responses
#                 if e.get('code', 0) >= 400 and 
#                 e.get('candidate_char', '') in SAFE_CHARS
#             }
#             if len(error_chars) == 1:
#                 return next(iter(error_chars.keys()))
#             return None

#         # Find max deviation
#         max_dev = max(dev for dev, _ in deviations)
        
#         # Get all chars with max deviation
#         probable_chars = set()
#         for dev, e in deviations:
#             if dev == max_dev:
#                 char = e.get('candidate_char', '')
#                 if char in SAFE_CHARS:
#                     probable_chars.add(char)
        
#         return next(iter(probable_chars)) if probable_chars else None

def find_successful_wordlist_payload(responses):
    """
    Identify the payload (e.g., password) that succeeded in a wordlist attack.
    Criteria (in order):
    1. HTTP status code in 200-299 (success)
    2. show_response True (passed filters, e.g. show_string)
    3. Longest deviation from majority response length
    Returns the payload string or None if not found.
    """
    if not responses:
        return None

    # 1. Check for HTTP success codes
    for r in responses:
        if 200 <= r.get('code', 0) < 300:
            return r.get('payload')

    # 2. Check for show_response
    for r in responses:
        if r.get('show_response'):
            return r.get('payload')

    # 3. Fallback: length deviation from majority
    lengths = [r.get('chars', 0) for r in responses]
    if not lengths:
        return None

    # majority pattern: most common length
    most_common_len, _ = Counter(lengths).most_common(1)[0]
    deviations = [ (abs(r['chars'] - most_common_len), r) for r in responses ]
    deviations.sort(reverse=True, key=lambda x: x[0])
    # return payload with max deviation
    return deviations[0][1].get('payload')

def analyze_responses(responses, time_based=False):
    """
    Unified analysis for fuzzing responses.
    - If time_based: delegate to timing analysis
    - If payloads are multi-char (wordlist), use dedicated finder
    - Else fallback to char-based analysis
    """
    # Detect multi-char payloads: any payload length > 1
    # multi = any(len(r.get('payload', '')) > 1 for r in responses)
    # if multi and not time_based:
    #     return find_successful_wordlist_payload(responses)

    # existing char-based logic
    if time_based:
        # ... existing time-based code ...
        from statistics import mean
        stats = {}
        for r in responses:
            c = r.get('payload', '')
            if len(c) != 1: continue
            stats.setdefault(c, []).append(r['response_time'])
        if not stats: return None
        avg = {c: sum(t)/len(t) for c, t in stats.items()}
        global_mean = mean(avg.values())
        best = max(avg.items(), key=lambda x: x[1] - global_mean)
        if (best[1] - global_mean) > global_mean * 0.35:
            return best[0]
        return None
    else:
        # existing content-deviation logic
        patterns = Counter((r['chars'], r['words'], r.get('lines',0)) for r in responses)
        majority, _ = patterns.most_common(1)[0]
        deviations = []
        for r in responses:
            if r.get('code', 0) < 400:
                patt = (r['chars'], r['words'], r.get('lines',0))
                if patt != majority:
                    dev = sum(abs(a-b) for a, b in zip(patt, majority))
                    deviations.append((dev, r))
        if not deviations:
            return None
        # pick payload with max deviation
        best = max(deviations, key=lambda x: x[0])[1]
        return best.get('payload')

## ----- Helper Functions ----- ##

def load_dynamic_script(script_path: str):
    """Load and return a module from the given script path"""
    if not os.path.exists(script_path):
        return None
    
    module_name = os.path.splitext(os.path.basename(script_path))[0]
    spec = importlib.util.spec_from_file_location(module_name, script_path)
    if not spec or not spec.loader:
        return None
    
    module = importlib.util.module_from_spec(spec)
    sys.modules[module_name] = module
    spec.loader.exec_module(module)
    return module

def execute_dynamic_script(script_module, data: Dict[str, Any]):
    global VERBOSE
    """Execute the dynamic script if conditions are met"""
    if not script_module:
        return False
    
    try:
        # Check if the script has a condition function
        if hasattr(script_module, 'condition'):
            condition_met = script_module.condition(data)
            if not condition_met and VERBOSE:
                print(f"Skipping script {script_module.__name__} due to condition not met.")
                return False
        
        # Execute the main function if it exists
        if hasattr(script_module, 'execute'):
            script_module.execute(data)
            if VERBOSE:
                print(f"Executed dynamic script: {script_module.__name__}")
            return True
        
        return False
    except Exception as e:
        print(f"Error executing dynamic script: {str(e)}")
        return False

def to_ansi_c_quoted_string(s: str) -> str:
    """Convert string to ANSI C quoted string for shell."""
    parts, i = [], 0
    while i < len(s):
        c = s[i]
        if c == '\r' and i + 1 < len(s) and s[i+1] == '\n':
            parts.append('\\x0d\\x0a'); i += 2; continue
        if c == '\n':
            parts.append('\\x0d\\x0a'); i += 1; continue
        if 32 <= ord(c) <= 126 and c not in "\\'":
            parts.append(c)
        else:
            if c == '\\': parts.append('\\\\')
            elif c == "'": parts.append("\\'")
            else: parts.append(f"\\x{ord(c):02x}")
        i += 1
    return "$'" + ''.join(parts) + "'"

def parse_request_file(path: str) -> Tuple[str, str, Dict[str, str], str]:
    """Parse Burp HTTP request file into method, path, headers, body."""
    with open(path, 'r') as f:
        lines = f.read().splitlines()
    method, req_path, _ = lines[0].split(maxsplit=2)
    headers, body_lines, in_body = {}, [], False
    for line in lines[1:]:
        if not in_body and not line.strip():
            in_body = True
            continue
        if in_body:
            body_lines.append(line)
        else:
            if ':' in line:
                k, v = line.split(':', 1)
                headers[k.strip()] = v.strip()

    base_url = headers.get('Host', '').strip()

    body = '\r\n'.join(body_lines)
    return method, req_path, headers, body, base_url

def parse_response():
    """Parse response from output."""
    raise NotImplementedError("This function is not implemented yet. It should parse the output and return structured data.")

## ----- Setup ----- ##

def parse_mode_options(raw_options: str) -> Dict[str, Any]:
    """
    Parse mode options from a string like 'set=alphanum,append=True'.
    Returns a dictionary of options.
    """
    options = {}
    for opt in raw_options.split(','):
        if '=' in opt:
            key, value = opt.split('=', 1)
            value = value.strip()
            if value.lower() == 'true':
                options[key.strip()] = True
            elif value.lower() == 'false':
                options[key.strip()] = False
            else:
                options[key.strip()] = value.strip()
        else:
            options[opt.strip()] = True

    return options

def parse_fuzz_tokens(text: str) -> List[Dict[str, Any]]:
    # Regex pattern to find fuzz tokens
    fuzz_pattern = re.compile(
        r'(F(?P<num>\d+)Z(?P<mode>[A-Z]+)(?::(?P<options>.*?))?:Z)'
    )

    results = []
    for match in fuzz_pattern.finditer(text):
        full_token = match.group(1)
        num = int(match.group("num"))
        mode = match.group("mode")
        options_raw = match.group("options") or ""

        # Start with default empty dict
        options: Dict[str, Any] = {}

        # Mode-specific option parsing
        if mode == "W":  # Wordlist mode
            # Match to default wordlist or custom wordlist
            options = {
                "wordlist": options_raw.strip()
            }

            for key, default in DEFAULT_WORDLISTS.items():
                if options_raw.strip() == key:
                    options["wordlist"] = os.path.expanduser(default)
                    break

        elif mode == "I":  # Integer guessing
            options = parse_mode_options(options_raw)

            # Defaults
            options.setdefault("start", "0")
            options.setdefault("step", "1")
            options.setdefault("end", "100")  # Default end value
            options.setdefault("follow", False)

        elif mode == "G":  # Character guessing (ZG:set=...)
            options = parse_mode_options(options_raw)
            options.setdefault("set", "alphanum")
            options.setdefault("append", False)

        else:
            # Fallback for unknown modes â€” raw options as key-value if possible
            for opt in options_raw.split(","):
                if "=" in opt:
                    key, value = opt.split("=", 1)
                    options[key.strip()] = value.strip()
                elif opt.strip():
                    options[opt.strip()] = True

        results.append({
            "token": full_token,
            "index": num,
            "mode": mode,
            "options": options
        })

    return results

def build_custom_parameters(method: str, path: str, headers: Dict[str, str], body: str, args: argparse.Namespace) -> Dict[str, Any]:
    """Build custom parameters for python."""
    headers_fuzz = parse_fuzz_tokens(json.dumps(headers))
    body_fuzz = parse_fuzz_tokens(body)
    path_fuzz = parse_fuzz_tokens(path)

    full_fuzz = headers_fuzz + body_fuzz + path_fuzz

    params = {
        'method': method,
        'headers': headers,
        'body': body,
        'base_url': args.base_url,
        'path': path,
        'response_filters' : {
            'hide_length': args.hide_length,
            'show_length': args.show_length,
            'show_code': args.show_code,
            'hide_code': args.hide_code,
            'show_string': args.show_string,
            'hide_string': args.hide_string
        },
        'FUZZ' : full_fuzz
    }
    return params

## ----- Fireing ----- ##

class TokenGenerator:
    def __init__(self, token_spec: Dict[str, Any]):
        self.token = token_spec['token']
        self.mode = token_spec['mode']
        self.options = token_spec.get('options', {})
        
        # State tracking
        self.guessed_length = 0  # For follow=True tokens
        self.current_value = None  # For I mode tokens
        self.current_string = ""  # For G mode tokens
        self.wordlist_index = 0  # For W mode tokens
        
        # Initialize based on mode
        if self.mode == 'I':
            self.start = int(self.options.get('start', 1))
            self.end = int(self.options.get('end', 100))
            self.step = int(self.options.get('step', 1))
            self.current_value = self.start
            self.follow = self.options.get('follow', False)
            
        elif self.mode == 'W':
            self.wordlist = self.options['wordlist']
            self.words = self._load_wordlist()
            
        elif self.mode == 'G':
            self.charset = self._get_charset()
            self.char_index = 0

    def _get_charset(self) -> str:
        """Get character set for G mode"""
        charset = self.options.get('set', 'alphanum')
        if charset == 'alphanum': 
            return string.ascii_letters + string.digits + '-_'
        elif charset == 'hex': 
            return string.hexdigits.lower()
        elif charset == 'numeric': 
            return string.digits
        return charset

    def _load_wordlist(self) -> list:
        """Load wordlist from file"""
        try:
            with open(self.wordlist, 'r', errors='replace') as f:
                return [line.strip() for line in f if line.strip()]
        except Exception:
            return []

    def get_payload(self) -> str:
        """Generate the next payload based on token mode"""
        if self.mode == 'I':
            if self.follow:
                return str(self.guessed_length)
            else:
                payload = str(self.current_value)
                if self.current_value <= self.end:
                    self.current_value += self.step
                return payload
                
        elif self.mode == 'W':
            if self.wordlist_index < len(self.words):
                payload = self.words[self.wordlist_index]
                self.wordlist_index += 1
                return payload
            return ""
            
        elif self.mode == 'G':
            if self.char_index < len(self.charset):
                char = self.charset[self.char_index]
                self.char_index += 1
                if self.options.get('append', False):
                    return self.current_string + char
                else:
                    return char
            return ""
        
        return ""

    def advance_position(self):
        """Advance to next character position for G mode"""
        if self.mode == 'G' and self.char_index >= len(self.charset):
            self.current_string += "?"
            self.char_index = 0

    def confirm_guess(self, guess: str):
        """Update state with successful guess"""
        if self.mode == 'G':
            self.current_string = guess
            self.char_index = 0

def fire_requests(parameters: Dict[str, Any], time_based: bool = False) -> List[Dict[str, Any]]:
    """
    Fire threaded fuzzing requests for both stateful and non-stateful tokens.
    - If any non-stateful payload triggers the filters, exit early.
    - If no filters are provided, exhaust all non-stateful payloads, then pick the one
      whose response length deviates most from the majority.
    - For stateful (G-mode) tokens, exit as soon as no new character can be distinguished,
      returning the final assembled string.
    """
    from concurrent.futures import ThreadPoolExecutor, as_completed

    method        = parameters['method']
    headers       = parameters['headers']
    body          = parameters['body']
    base_url      = parameters['base_url']
    path          = parameters['path']
    req_url       = f"{base_url}{path}"
    filters       = parameters.get('response_filters', {}) or {}
    fuzz_tokens   = parameters.get('FUZZ', [])
    script_modules= parameters.get('script_modules', [])

    # Initialize generators for each FUZZ token
    token_generators = {}
    for spec in fuzz_tokens:
        try:
            token_generators[spec['token']] = TokenGenerator(spec)
        except Exception as e:
            print(f"Error creating token generator: {e}")
            return []

    # Categorize tokens
    stateful      = [t for t, g in token_generators.items() if g.mode == 'G']
    non_stateful  = [t for t, g in token_generators.items() if g.mode != 'G']
    follow_tokens = {t: g for t, g in token_generators.items() if g.mode == 'I' and g.follow}

    results = []
    all_nonstateful_resps = []

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # If there are no stateful tokens, first exhaust all non-stateful payloads
        if not stateful:
            nonstateful_futures: Dict = {}
            while True:
                # build next payload set
                repl = {t: token_generators[t].get_payload() for t in non_stateful}
                # stop when any generator returns empty string
                if any(val == "" for val in repl.values()):
                    break

                # prepare headers/body
                new_h = {k: v for k, v in headers.items()}
                for k, v in new_h.items():
                    for tok, val in repl.items():
                        v = v.replace(tok, val)
                    new_h[k] = v
                new_b = body
                for tok, val in repl.items():
                    new_b = new_b.replace(tok, val)

                # submit request
                fut = executor.submit(
                    send_request,
                    req_url, method, new_h, new_b,
                    filters,
                    payload=str(repl),
                    script_modules=script_modules
                )
                nonstateful_futures[fut] = repl

            # collect all non-stateful responses
            for fut in as_completed(nonstateful_futures):
                repl = nonstateful_futures[fut]
                resp = fut.result()[0]
                all_nonstateful_resps.append((repl, resp))
                results.append(resp)
                # if any filter is active and this resp passes, exit immediately
                if any(v is not None for v in filters.values()) and resp.get('show_response'):
                    print(f"\nðŸ”‘ Found matching non-stateful payload: {repl}")
                    return [resp]

        # Otherwise, or after exhausting non-stateful, handle stateful character guessing
        while stateful:
            # update followâ€‘length tokens
            for t, g in follow_tokens.items():
                g.guessed_length = max(len(token_generators[s].current_string) for s in stateful)

            futures: Dict = {}
            # submit one request per candidate character per token
            for t in stateful:
                gen = token_generators[t]
                if len(gen.current_string) >= 50:
                    continue
                for idx, ch in enumerate(gen.charset):
                    if gen.options.get('append', False) == True:
                        candidate = gen.current_string + ch
                    else:
                        candidate = ch

                    repl = {x: token_generators[x].get_payload() for x in non_stateful}
                    repl[t] = candidate

                    new_h = {k: v for k, v in headers.items()}
                    for k, v in new_h.items():
                        for tok, val in repl.items():
                            v = v.replace(tok, val)
                        new_h[k] = v
                    new_b = body
                    for tok, val in repl.items():
                        new_b = new_b.replace(tok, val)

                    fut = executor.submit(
                        send_request,
                        req_url, method, new_h, new_b,
                        filters,
                        idx=idx, payload=candidate,
                        script_modules=script_modules
                    )
                    futures[fut] = (t, ch)

            # collect responses
            grouped = {t: [] for t in stateful}
            for fut in as_completed(futures):
                resp = fut.result()[0]
                t, ch = futures[fut]
                resp.update({'candidate_char': ch, 'token': t})
                grouped[t].append(resp)
                results.append(resp)

            # analyze and update each stateful token
            progress = False
            for t in list(stateful):
                grp = grouped.get(t, [])
                if not grp:
                    continue
                probable = analyze_responses(grp, time_based)
                gen = token_generators[t]
                if probable:
                    if gen.options.get('append', False):
                        gen.current_string = probable
                    else:
                        gen.current_string = gen.current_string + probable
                    print(f"\nâœ… Token '{t}' extended to: {gen.current_string}")
                    progress = True
                else:
                    # no distinguishing char â†’ final value reached
                    final_val = gen.current_string
                    print(f"\nðŸ Token '{t}' final value: {final_val}")
                    stateful.remove(t)

            if not progress:
                break  # no further progress

    # Postâ€processing for pure nonâ€stateful when no filters were provided
    if not stateful and all(v is None for v in filters.values()) and all_nonstateful_resps:
        # Extract just the response dicts
        flat_resps = [resp for _, resp in all_nonstateful_resps]

        # Delegate to our unified analyzer (will pick the 2xx or bestâ€‘length payload)
        successful = find_successful_wordlist_payload(flat_resps, time_based=False)

        if successful:
            # Find and return only the matching response(s)
            matched = [r for r in flat_resps if r.get('payload') == successful]
            print(f"\\nðŸ”‘ Wordlist attack succeeded with payload: {successful}")
            return matched
        else:
            print("\\nâš ï¸ No successful payload found in wordlist attack.")

    return results

def send_request(
    req_url: str, 
    method: str, 
    headers: Dict[str, str], 
    body: str, 
    filters: Dict[str, Any],
    idx: int = 0,
    payload: str = "",
    script_modules: Optional[List[Any]] = None,
    print_response: bool = False
) -> Tuple[bool, Dict[str, Any]]:
    global VERBOSE
    """Send a request and return filter status + summary."""
    try:
        response = requests.request(
            method, req_url, headers=headers, data=body, timeout=10
        )
        if print_response:
            print(f"{response.text}")

        show_response = True
        if VERBOSE:
            print(f"ðŸ”¥ Sending request #{idx + 1} to {req_url} with payload: {payload}, idx: {idx}")
        # Apply response filters
        for key, filter_val in filters.items():
            if filter_val is None:
                continue
                
            response_text = response.text
            
            if key == 'hide_length' and response_text:
                if len(response_text) == filter_val:
                    show_response = False
            elif key == 'show_length' and response_text:
                if len(response_text) != filter_val:
                    show_response = False
            elif key == 'show_code' and response.status_code != filter_val:
                show_response = False
            elif key == 'hide_code' and response.status_code == filter_val:
                show_response = False
            elif key == 'show_string' and filter_val not in response_text:
                show_response = False
            elif key == 'hide_string' and filter_val in response_text:
                show_response = False
        
        summary = {
            'code': response.status_code,
            'chars': len(response.text),
            'response_time': response.elapsed.total_seconds(),
            'payload': payload,
            'words': len(response.text.split()),
            'idx' : idx,
            'show_response': show_response
        }
        if script_modules:
            for script_module in script_modules:
                execute_dynamic_script(script_module, summary)

        return [summary]
    
    except requests.RequestException as e:
        print(f"Request failed: {e}")
        return [{
            'code': 500, 
            'chars': 0, 
            'words': 0, 
            'lines': 0, 
            'response_time': 0,
            'payload': body,
            'show_response': False
        }]
    
def main() -> None:
    global VERBOSE
    global MAX_WORKERS

    parser = argparse.ArgumentParser(description="Convert request file to fuzzing tool.")
    subparsers = parser.add_subparsers(dest="command", required=True)
    
    run_parser = subparsers.add_parser("run", help="Payload-related operations")
    run_parser.add_argument("request_file")

    run_parser.add_argument("--base_url")

    run_parser.add_argument("--hide-length", "--hl", type=int)
    run_parser.add_argument("--show-length", "--sh", type=int)
    run_parser.add_argument("--show-code", "--sc", type=int, help="Show response code in the output")
    run_parser.add_argument("--hide-code", "--hc", type=int, help="Hide response code in the output")
    run_parser.add_argument("--show-string", "--ss", type=str, help="Show the string in the output")
    run_parser.add_argument("--hide-string", "--hs", type=str, help="Hide the string in the output")

    run_parser.add_argument("--script", action="append", help="Conditional scripts to run on the response")

    run_parser.add_argument("--https", action="store_true") # HTTP per default unless specified
        
    run_parser.add_argument("--time-based", action="store_true", help="Enable time-based guessing")
    run_parser.add_argument("--time-based-deviation", type=float, default=0.35, help="Deviation threshold for time-based guessing (default: 0.35)")

    run_parser.add_argument("--max-workers", type=int, default=MAX_WORKERS, help="Maximum number of concurrent workers (default: 10)")

    run_parser.add_argument("--test-single", action="store_true", help="Test single request with the given parameters and print the response")

    run_parser.add_argument("--dry-run", "--dr", action="store_true", help="Dry run: show parsed parameters without executing requests")
    run_parser.add_argument("-v", action="store_true", help="Verbose mode: print more details")

    args = parser.parse_args()

    if args.v:
        VERBOSE = True

    if args.command == "run":
        if VERBOSE:
            print(f"Running with arguments: {args}")
        
        if args.max_workers:
            MAX_WORKERS = args.max_workers
            if VERBOSE:
                print(f"Setting max workers to {MAX_WORKERS}")
            
        method, path, headers, body, base_url = parse_request_file(args.request_file)
        if not base_url and not args.base_url:
            raise ValueError("Host header not found in request.")
        else:
            if not args.base_url:
                if args.https:
                    args.base_url = f"https://{base_url}"
                else:
                    args.base_url = f"http://{base_url}"
        
        if not path.startswith('/'):
            path = '/' + path

        params = build_custom_parameters(method, path, headers, body, args)

        if VERBOSE and not args.test_single:
            print(f"Parsed parameters: {json.dumps(params, indent=2)}")
        
        if args.dry_run:
            if not VERBOSE:
                print(f"Parsed parameters: {json.dumps(params, indent=2)}")

            print("Dry run mode: not executing requests.")
            return

        script_modules = []
        if args.script:
            for script_path in args.script:
                script_module = load_dynamic_script(script_path)
                if script_module:
                    print(f"Loaded dynamic script: {script_path}")
                    script_modules.append(script_module)
                else:
                    print(f"Failed to load dynamic script: {script_path}")
        params['script_modules'] = script_modules

        if args.test_single:
            # Test single request with the given parameters
            print("Testing single request with parameters:")

            # Get first word of the wordlist if available
            if 'FUZZ' in params and params['FUZZ']:
                for token in params['FUZZ']:
                    if token['mode'] == 'W' and 'wordlist' in token['options']:
                        wordlist_path = token['options']['wordlist']
                        if os.path.exists(wordlist_path):
                            with open(wordlist_path, 'r') as f:
                                first_word = f.readline().strip()
                                params['body'] = params['body'].replace(token['token'], first_word)
                                params['headers'] = {k: v.replace(token['token'], first_word) for k, v in params['headers'].items()}
                                params['path'] = params['path'].replace(token['token'], first_word)
                                
                    elif token['mode'] == 'I' and 'start' in token['options']:
                        # For integer tokens, replace with the start value
                        start_value = token['options'].get('start', '0')
                        params['body'] = params['body'].replace(token['token'], start_value)
                        params['headers'] = {k: v.replace(token['token'], start_value) for k, v in params['headers'].items()}
                        params['path'] = params['path'].replace(token['token'], start_value)

                    elif token['mode'] == 'G' and 'set' in token['options']:
                        # For character guessing tokens, replace with the first character of the set
                        charset = token['options'].get('set', 'alphanum')
                        first_char = charset[0] if charset else '?'
                        params['body'] = params['body'].replace(token['token'], first_char)
                        params['headers'] = {k: v.replace(token['token'], first_char) for k, v in params['headers'].items()}
                        params['path'] = params['path'].replace(token['token'], first_char)

            else:
                print("No FUZZ tokens found in parameters, using original body and headers.")
            filtered_params = {k: v for k, v in params.items() if k != 'script_modules'}

            print(json.dumps(filtered_params, indent=2))

            response = send_request(
                f"{params['base_url']}{params['path']}", params['method'], params['headers'], 
                params['body'], params['response_filters'], print_response=True,
            )
            print(f"Response: {json.dumps(response, indent=2)}")
            return
        
        result = fire_requests(params, time_based=args.time_based)
        if VERBOSE:
            print(f"Results: {json.dumps(result, indent=2)}")

if __name__ == "__main__":
    main()
