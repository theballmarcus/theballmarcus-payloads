#!/usr/bin/env python3
"""
Fuzz Helper Tool

Converts a HTTP(S) request file into fuzzing requests.
and optionally performs character-by-character fuzzing.
"""
import argparse
import json
import os
import string
import sys
from collections import Counter
from typing import Any, Dict, List, Tuple, Union, Generator, Optional
from unittest import result
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
import statistics
import importlib.util
import re

VERBOSE = False
MAX_WORKERS = 10  # Default max workers for ThreadPoolExecutor
DEFAULT_WORDLISTS = {
    'rockyou' : '~/wordlist/rockyou.txt',
    'directories' : '~/wordlist/directory-list-2.3-small.txt',
    'usernames' : '~/wordlist/usernames.txt',
}
SAFE_CHARS = set(string.ascii_letters + string.digits + '-_')

## ----- Response Analysis ----- ##

def find_most_probable_character(data: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
    """
    Identify deviations from the majority response pattern to find probable characters.
    Returns dict with 'safe_chars' and 'special_chars'.
    If no deviations are found, but only one safe_char appears in error responses, assume it's the probable character.
    """
    pattern_counts = Counter((e['chars'], e['words'], e['lines']) for e in data)
    if not pattern_counts:
        return {'safe_chars': [], 'special_chars': []}

    majority_pattern, _ = pattern_counts.most_common(1)[0]
    deviations = []

    for e in data:
        if e.get('code', 0) < 400:
            patt = (e['chars'], e['words'], e['lines'])
            if patt != majority_pattern:
                dev = sum(abs(a - b) for a, b in zip(patt, majority_pattern))
                deviations.append((dev, e))

    deviations.sort(key=lambda x: x[0], reverse=True)
    result = {'safe_chars': [], 'special_chars': []}

    if deviations:
        max_dev = deviations[0][0]
        for dev, e in deviations:
            if dev != max_dev:
                break
            char = e.get('payload', '')
            details = {'chars': e['chars'], 'words': e['words'], 'lines': e['lines'], 'deviation': dev}
            key = 'safe_chars' if len(char) == 1 and char in SAFE_CHARS else 'special_chars'
            result[key].append({'char': char, 'details': details})

    # Fallback: If no deviations matched, check for single unique safe_char among errors
    if not result['safe_chars'] and not result['special_chars']:
        error_safe_chars = {
            e.get('payload', ''): e for e in data
            if e.get('code', 0) >= 400 and len(e.get('payload', '')) == 1 and e.get('payload') in SAFE_CHARS
        }
        if len(error_safe_chars) == 1:
            char, e = next(iter(error_safe_chars.items()))
            details = {'chars': e['chars'], 'words': e['words'], 'lines': e['lines'], 'deviation': None}
            result['safe_chars'].append({'char': char, 'details': details})

    return result

def find_most_probable_time_based_character(
    data: List[Dict[str, Any]],
    deviation_threshold: float = 1  # 35% by default
) -> Union[Dict[str, Any], bool]:
    """
    Identify the character with the largest positive deviation in response time from the mean.
    Return False if no character deviates enough (based on deviation_threshold).
    
    Parameters:
        - data: list of request/response records
        - deviation_threshold: minimum percent deviation from mean to be considered significant

    Returns:
        - dict with 'most_probable', 'deviation', and 'all_char_stats', or
        - False if no character significantly deviates
    """
    if not data:
        return False

    # Group by payloads
    payload_groups = {}
    for e in data:
        payload = e.get('payload', '')
        if len(payload) != 1:
            continue  # Skip multi-char payloads
        payload_groups.setdefault(payload, []).append(e)

    char_stats = []
    
    for payload, group in payload_groups.items():
        avg_response_time = sum(e['response_time'] for e in group) / len(group)
        stats = {
            'char': payload,
            'response_time': avg_response_time,
            'count': len(group),
            'details': {
                'chars': sum(e['chars'] for e in group),
                'words': sum(e['words'] for e in group),
                'lines': sum(e['lines'] for e in group),
            }
        }
        char_stats.append(stats)

    if not char_stats:
        return False

    # Compute global average
    global_mean = statistics.mean(c['response_time'] for c in char_stats)

    # Add deviation to each
    for c in char_stats:
        c['deviation'] = c['response_time'] - global_mean
        c['relative_deviation'] = c['deviation'] / global_mean if global_mean else 0

    # Get the max deviation entry
    most_probable = max(char_stats, key=lambda x: x['deviation'])
    print(f"Most probable character: {most_probable['char']} with deviation {most_probable['deviation']} and relative deviation {most_probable['relative_deviation']:.2%}")
    # Check if the deviation is significant enough
    if most_probable['relative_deviation'] < deviation_threshold:
        return False

    return {
        'most_probable': most_probable['char'],
        'deviation': most_probable['deviation'],
        'relative_deviation': most_probable['relative_deviation'],
        'all_char_stats': char_stats
    }

def analyze_responses(responses: List[Dict[str, Any]], time_based: bool) -> Optional[str]:
    """
    Analyze responses to find the most probable character.
    Uses different strategies based on attack type.
    """
    if not responses:
        return None
    
    if time_based:
        # Group by payload characters
        char_groups = {}
        for r in responses:
            char = r.get('candidate_char', '')
            if char:
                char_groups.setdefault(char, []).append(r)
        
        # Calculate average response times
        char_stats = []
        for char, group in char_groups.items():
            avg_time = sum(r['response_time'] for r in group) / len(group)
            char_stats.append({'char': char, 'avg_time': avg_time})
        
        if not char_stats:
            return None
        
        # Find global average
        global_avg = statistics.mean(s['avg_time'] for s in char_stats)
        
        # Find character with max deviation
        max_deviation = 0
        probable_char = None
        for stat in char_stats:
            deviation = stat['avg_time'] - global_avg
            if deviation > max_deviation:
                max_deviation = deviation
                probable_char = stat['char']
                
        # Check if deviation is significant (35% threshold)
        if max_deviation > global_avg * 0.35:
            return probable_char
        return None
    
    else:
        # Non-time-based analysis
        pattern_counts = Counter(
            (e['chars'], e['words'], e['lines']) for e in responses
        )
        if not pattern_counts:
            return None

        majority_pattern, _ = pattern_counts.most_common(1)[0]
        deviations = []

        for e in responses:
            if e.get('code', 0) < 400:
                pattern = (e['chars'], e['words'], e['lines'])
                if pattern != majority_pattern:
                    dev = sum(abs(a - b) for a, b in zip(pattern, majority_pattern))
                    deviations.append((dev, e))

        if not deviations:
            # Fallback: look for single safe char in error responses
            error_chars = {
                e.get('candidate_char', ''): e for e in responses
                if e.get('code', 0) >= 400 and 
                e.get('candidate_char', '') in SAFE_CHARS
            }
            if len(error_chars) == 1:
                return next(iter(error_chars.keys()))
            return None

        # Find max deviation
        max_dev = max(dev for dev, _ in deviations)
        
        # Get all chars with max deviation
        probable_chars = set()
        for dev, e in deviations:
            if dev == max_dev:
                char = e.get('candidate_char', '')
                if char in SAFE_CHARS:
                    probable_chars.add(char)
        
        return next(iter(probable_chars)) if probable_chars else None

## ----- Helper Functions ----- ##

def load_dynamic_script(script_path: str):
    """Load and return a module from the given script path"""
    if not os.path.exists(script_path):
        return None
    
    module_name = os.path.splitext(os.path.basename(script_path))[0]
    spec = importlib.util.spec_from_file_location(module_name, script_path)
    if not spec or not spec.loader:
        return None
    
    module = importlib.util.module_from_spec(spec)
    sys.modules[module_name] = module
    spec.loader.exec_module(module)
    return module

def execute_dynamic_script(script_module, data: Dict[str, Any]):
    """Execute the dynamic script if conditions are met"""
    if not script_module:
        return False
    
    try:
        # Check if the script has a condition function
        if hasattr(script_module, 'condition'):
            condition_met = script_module.condition(data)
            if not condition_met:
                return False
        
        # Execute the main function if it exists
        if hasattr(script_module, 'execute'):
            script_module.execute(data)
            return True
        
        return False
    except Exception as e:
        print(f"Error executing dynamic script: {str(e)}")
        return False

def to_ansi_c_quoted_string(s: str) -> str:
    """Convert string to ANSI C quoted string for shell."""
    parts, i = [], 0
    while i < len(s):
        c = s[i]
        if c == '\r' and i + 1 < len(s) and s[i+1] == '\n':
            parts.append('\\x0d\\x0a'); i += 2; continue
        if c == '\n':
            parts.append('\\x0d\\x0a'); i += 1; continue
        if 32 <= ord(c) <= 126 and c not in "\\'":
            parts.append(c)
        else:
            if c == '\\': parts.append('\\\\')
            elif c == "'": parts.append("\\'")
            else: parts.append(f"\\x{ord(c):02x}")
        i += 1
    return "$'" + ''.join(parts) + "'"

def parse_request_file(path: str) -> Tuple[str, str, Dict[str, str], str]:
    """Parse Burp HTTP request file into method, path, headers, body."""
    with open(path, 'r') as f:
        lines = f.read().splitlines()
    method, req_path, _ = lines[0].split(maxsplit=2)
    headers, body_lines, in_body = {}, [], False
    for line in lines[1:]:
        if not in_body and not line.strip():
            in_body = True
            continue
        if in_body:
            body_lines.append(line)
        else:
            if ':' in line:
                k, v = line.split(':', 1)
                headers[k.strip()] = v.strip()

    base_url = headers.get('Host', '').strip()

    body = '\r\n'.join(body_lines)
    return method, req_path, headers, body, base_url

def parse_response():
    """Parse response from output."""
    raise NotImplementedError("This function is not implemented yet. It should parse the output and return structured data.")

## ----- Setup ----- ##

def parse_fuzz_tokens(text: str) -> List[Dict[str, Any]]:
    # Regex pattern to find fuzz tokens
    fuzz_pattern = re.compile(
        r'(F(?P<num>\d+)Z(?P<mode>[A-Z]+)(?::(?P<options>.*?))?:Z)'
    )

    results = []
    for match in fuzz_pattern.finditer(text):
        full_token = match.group(1)
        num = int(match.group("num"))
        mode = match.group("mode")
        options_raw = match.group("options") or ""

        # Start with default empty dict
        options: Dict[str, Any] = {}

        # Mode-specific option parsing
        if mode == "W":  # Wordlist mode
            # Match to default wordlist or custom wordlist

            options = {
                "wordlist": options_raw.strip()
            }

            for key, default in DEFAULT_WORDLISTS.items():
                if options_raw.strip() == key:
                    options["wordlist"] = os.path.expanduser(default)
                    break

        elif mode == "I":  # Integer guessing
            for opt in options_raw.split(","):
                if "=" in opt:
                    key, value = opt.split("=", 1)
                    options[key.strip()] = value.strip()
                else:
                    options[opt.strip()] = True
                

            # Defaults
            options.setdefault("start", "0")
            options.setdefault("step", "1")
            options.setdefault("end", "100")  # Default end value
            options.setdefault("follow", False)

        elif mode == "G":  # Character guessing (ZG:set=...)
            for opt in options_raw.split(","):
                if "=" in opt:
                    key, value = opt.split("=", 1)
                    options[key.strip()] = value.strip()
            options.setdefault("set", "alphanum")

        else:
            # Fallback for unknown modes — raw options as key-value if possible
            for opt in options_raw.split(","):
                if "=" in opt:
                    key, value = opt.split("=", 1)
                    options[key.strip()] = value.strip()
                elif opt.strip():
                    options[opt.strip()] = True

        results.append({
            "token": full_token,
            "index": num,
            "mode": mode,
            "options": options
        })

    return results

def build_custom_parameters(method: str, path: str, headers: Dict[str, str], body: str, args: argparse.Namespace) -> Dict[str, Any]:
    """Build custom parameters for python."""
    headers_fuzz = parse_fuzz_tokens(json.dumps(headers))
    body_fuzz = parse_fuzz_tokens(body)
    path_fuzz = parse_fuzz_tokens(path)

    full_fuzz = headers_fuzz + body_fuzz + path_fuzz

    params = {
        'method': method,
        'headers': headers,
        'body': body,
        'base_url': args.base_url,
        'path': path,
        'response_filters' : {
            'hide_length': args.hide_length,
            'show_length': args.show_length,
            'show_code': args.show_code,
            'hide_code': args.hide_code,
            'show_string': args.show_string,
            'hide_string': args.hide_string
        },
        'FUZZ' : full_fuzz
    }
    return params

## ----- Fireing ----- ##

class TokenGenerator:
    def __init__(self, token_spec: Dict[str, Any]):
        self.token = token_spec['token']
        self.mode = token_spec['mode']
        self.options = token_spec.get('options', {})
        self.follow = token_spec.get('follow', False)
        
        # State tracking
        self.guessed_length = 0  # For follow=True tokens
        self.current_value = None  # For I mode tokens
        self.current_string = ""  # For G mode tokens
        self.wordlist_index = 0  # For W mode tokens
        
        # Initialize based on mode
        if self.mode == 'I':
            self.start = int(self.options.get('start', 1))
            self.end = int(self.options.get('end', 100))
            self.step = int(self.options.get('step', 1))
            self.current_value = self.start
            
        elif self.mode == 'W':
            self.wordlist = self.options['wordlist']
            self.words = self._load_wordlist()
            
        elif self.mode == 'G':
            self.charset = self._get_charset()
            self.char_index = 0

    def _get_charset(self) -> str:
        """Get character set for G mode"""
        charset = self.options.get('set', 'alphanum')
        if charset == 'alphanum': 
            return string.ascii_letters + string.digits
        elif charset == 'hex': 
            return string.hexdigits.lower()
        elif charset == 'numeric': 
            return string.digits
        return charset

    def _load_wordlist(self) -> list:
        """Load wordlist from file"""
        try:
            with open(self.wordlist, 'r', errors='replace') as f:
                return [line.strip() for line in f if line.strip()]
        except Exception:
            return []

    def get_payload(self) -> str:
        """Generate the next payload based on token mode"""
        if self.mode == 'I':
            if self.follow:
                return str(self.guessed_length)
            else:
                payload = str(self.current_value)
                if self.current_value <= self.end:
                    self.current_value += self.step
                return payload
                
        elif self.mode == 'W':
            if self.wordlist_index < len(self.words):
                payload = self.words[self.wordlist_index]
                self.wordlist_index += 1
                return payload
            return ""
            
        elif self.mode == 'G':
            if self.char_index < len(self.charset):
                char = self.charset[self.char_index]
                self.char_index += 1
                return self.current_string + char
            return ""
        
        return ""

    def advance_position(self):
        """Advance to next character position for G mode"""
        if self.mode == 'G' and self.char_index >= len(self.charset):
            self.current_string += "?"
            self.char_index = 0

    def confirm_guess(self, guess: str):
        """Update state with successful guess"""
        if self.mode == 'G':
            self.current_string = guess
            self.char_index = 0

def fire_requests(parameters: Dict[str, Any], time_based: bool = False) -> List[Dict[str, Any]]:
    """Fire requests with integrated character analysis."""
    # Extract parameters
    method = parameters['method']
    headers = parameters['headers']
    body = parameters['body']
    base_url = parameters['base_url']
    path = parameters['path']
    req_url = f"{base_url}{path}"
    response_filters = parameters['response_filters']
    fuzz_tokens = parameters.get('FUZZ', [])
    script_modules = parameters.get('script_modules', [])
    
    print(f"Fuzz tokens found: {fuzz_tokens}")
    
    # Create token generators
    token_generators = {}
    for token_spec in fuzz_tokens:
        try:
            token_generators[token_spec['token']] = TokenGenerator(token_spec)
        except Exception as e:
            print(f"Error creating generator: {str(e)}")
            return []
    
    # Track stateful tokens
    stateful_tokens = [token for token, gen in token_generators.items() if gen.mode == 'G']
    non_stateful_tokens = [token for token, gen in token_generators.items() if gen.mode != 'G']
    follow_tokens = {token: gen for token, gen in token_generators.items() 
                    if gen.mode == 'I' and gen.follow}
    
    results = []
    request_count = 0
    
    # Create thread pool
    # max_workers = 10 if not time_based else 10
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        while True:
            # Update follow tokens with current guessed lengths
            for token, gen in follow_tokens.items():
                gen.guessed_length = max(
                    len(token_generators[t].current_string) for t in stateful_tokens
                ) if stateful_tokens else 0
            
            # Get base replacements for non-stateful tokens
            base_replacements = {}
            try:
                for token in non_stateful_tokens:
                    base_replacements[token] = token_generators[token].get_payload()
            except Exception:
                break  # Exit when non-stateful tokens are exhausted
            
            # Process stateful tokens
            for token in stateful_tokens:
                gen = token_generators[token]
                current_length = len(gen.current_string)
                
                # Skip if max length reached
                if current_length >= 50:
                    continue
                
                # Prepare all candidate characters
                responses = []
                futures = {}
                charset = gen.charset

                for idx, char in enumerate(charset):
                    # Generate candidate payload
                    candidate = gen.current_string + char
                    token_replacements = base_replacements.copy()
                    token_replacements[token] = candidate
                    
                    # Prepare request
                    new_headers = headers.copy()
                    for key, value in new_headers.items():
                        for t, repl in token_replacements.items():
                            value = value.replace(t, repl)
                        new_headers[key] = value
                    
                    new_body = body
                    for t, repl in token_replacements.items():
                        new_body = new_body.replace(t, repl)
                    
                    # Submit request
                    future = executor.submit(
                        send_request,
                        req_url, method, new_headers, new_body, 
                        response_filters,
                        idx=idx, payload=candidate,
                        script_modules=script_modules,
                    )
                    futures[future] = {
                        'char': char,
                        'candidate': candidate,
                        'token': token
                    }
                # Collect all responses
                for future in as_completed(futures):
                    try:
                        response = future.result()[0]

                        meta = futures[future]
                        
                        # Add analysis metadata
                        response.update({
                            'candidate_char': meta['char'],
                            'candidate_string': meta['candidate'],
                            'token': meta['token']
                        })
                        
                        responses.append(response)
                        request_count += 1
                        # print(f"\r🔥 Request #{request_count}: {meta['char']} ", end="")
                    except Exception as e:
                        print(f"\nRequest error: {str(e)}")
                
                # Analyze responses to find probable character
                probable_char = analyze_responses(responses, time_based)
                
                if probable_char:
                    # Confirm successful guess
                    full_guess = gen.current_string + probable_char
                    gen.confirm_guess(full_guess)
                    print(f"\n✅ Found char '{probable_char}' at position {current_length}")
                else:
                    # Advance to next position
                    gen.advance_position()
                    print(f"\nℹ️ Advanced to position {current_length + 1}")
            
            # Send verification request
            final_replacements = base_replacements.copy()
            for token in stateful_tokens:
                final_replacements[token] = token_generators[token].current_string
            
            new_headers = headers.copy()
            for key, value in new_headers.items():
                for token, repl in final_replacements.items():
                    value = value.replace(token, repl)
                new_headers[key] = value
            
            new_body = body
            for token, repl in final_replacements.items():
                new_body = new_body.replace(token, repl)
            
            print(f"\n🚀 Verification request with state: {final_replacements}")
            response = send_request(
                req_url, method, new_headers, new_body, response_filters, time_based
            )
            results.extend(response)
            request_count += 1
    
    print("\n✅ All tokens processed")
    return results

def send_request(
    req_url: str, 
    method: str, 
    headers: Dict[str, str], 
    body: str, 
    filters: Dict[str, Any],
    idx: int = 0,
    payload: str = "",
    script_modules: Optional[List[Any]] = None
) -> Tuple[bool, Dict[str, Any]]:
    global VERBOSE
    """Send a request and return filter status + summary."""
    try:
        response = requests.request(
            method, req_url, headers=headers, data=body, timeout=10
        )
        show_response = True
        if VERBOSE:
            print(f"🔥 Sending request #{idx + 1} to {req_url} with payload: {payload}")
        # Apply response filters
        for key, filter_val in filters.items():
            if filter_val is None:
                continue
                
            response_text = response.text
            
            if key == 'hide_length' and response_text:
                if len(response_text) == filter_val:
                    show_response = False
            elif key == 'show_length' and response_text:
                if len(response_text) != filter_val:
                    show_response = False
            elif key == 'show_code' and response.status_code != filter_val:
                show_response = False
            elif key == 'hide_code' and response.status_code == filter_val:
                show_response = False
            elif key == 'show_string' and filter_val not in response_text:
                show_response = False
            elif key == 'hide_string' and filter_val in response_text:
                show_response = False
        
        summary = {
            'code': response.status_code,
            'chars': len(response.text),
            'response_time': response.elapsed.total_seconds(),
            'payload': payload,
            'words': len(response.text.split()),
            'idx' : idx,
            'show_response': show_response
        }
        if script_modules:
            for script_module in script_modules:
                execute_dynamic_script(script_module, summary)
                
        return [summary]
    
    except requests.RequestException as e:
        print(f"Request failed: {e}")
        return [{
            'code': 500, 
            'chars': 0, 
            'words': 0, 
            'lines': 0, 
            'response_time': 0,
            'payload': body,
            'show_response': False
        }]
    
def guess_time_based(parameters: Dict[str, Any], deviation: float) -> str:
    """Build parameters for time-based guessing."""
    guessed_string = ""

    wordlist = open(parameters['wordlist'], 'r').read().splitlines()
    method = parameters['method']
    headers = parameters['headers']
    body = parameters['body']
    base_url = parameters['base_url']
    req_url = f"{base_url}{parameters['path']}"

    while True:
        print(f"\n🔥 Guessing next character with current string: '{guessed_string}'")
        results = []

        with ThreadPoolExecutor() as executor:
            futures = {}
            if 'FIZZIR' in req_url or 'FIZZIR' in body or any('FIZZIR' in v for v in headers.values()):
                futures = {
                    executor.submit(async_send_request, req_url, method, headers, body, word, idx + 1): word
                    for idx, word in enumerate(wordlist)
                }
            else:
                futures = {executor.submit(async_send_request, req_url, method, headers, body, word, len(guessed_string)): word for word in wordlist}
            for future in as_completed(futures):
                word = futures[future]
                try:
                    response = future.result()
                    if response:
                        results.extend(response)
                except Exception as e:
                    print(f"Error processing word '{word}': {e}")

        if not results:
            return "No results found."
        chars = find_most_probable_time_based_character(results, deviation)
        if chars != False:
            guessed_string += chars['most_probable']
            print(f"Guessed character: '{chars['most_probable']}' -> Current string: '{guessed_string}'")
        else:
            print("No significant deviation found. Stopping guessing.")
            print("Final guessed string:", guessed_string)
            break

def main() -> None:
    global VERBOSE

    parser = argparse.ArgumentParser(description="Convert request file to fuzzing tool.")
    subparsers = parser.add_subparsers(dest="command", required=True)
    
    run_parser = subparsers.add_parser("run", help="Payload-related operations")
    run_parser.add_argument("request_file")

    run_parser.add_argument("--base_url")

    run_parser.add_argument("--hide-length", "--hl", type=int)
    run_parser.add_argument("--show-length", "--sh", type=int)
    run_parser.add_argument("--show-code", "--sc", type=int, help="Show response code in the output")
    run_parser.add_argument("--hide-code", "--hc", type=int, help="Hide response code in the output")
    run_parser.add_argument("--show-string", "--ss", type=str, help="Show the string in the output")
    run_parser.add_argument("--hide-string", "--hs", type=str, help="Hide the string in the output")

    run_parser.add_argument("--script", action="append", help="Conditional scripts to run on the response")

    run_parser.add_argument("--https", action="store_true") # HTTP per default unless specified
    
    run_parser.add_argument("--time-based", action="store_true", help="Enable time-based guessing")
    run_parser.add_argument("--time-based-deviation", type=float, default=0.35, help="Deviation threshold for time-based guessing (default: 0.35)")

    run_parser.add_argument("--dry-run", "--dr", action="store_true", help="Dry run: show parsed parameters without executing requests")

    run_parser.add_argument("-v", action="store_true", help="Verbose mode: print more details")
    args = parser.parse_args()

    if args.v:
        VERBOSE = True

    if args.command == "run":
        if VERBOSE:
            print(f"Running with arguments: {args}")
            
        method, path, headers, body, base_url = parse_request_file(args.request_file)
        if not base_url and not args.base_url:
            raise ValueError("Host header not found in request.")
        else:
            if not args.base_url:
                if args.https:
                    args.base_url = f"https://{base_url}"
                else:
                    args.base_url = f"http://{base_url}"
        
        if not path.startswith('/'):
            path = '/' + path

        params = build_custom_parameters(method, path, headers, body, args)

        if VERBOSE:
            print(f"Parsed parameters: {json.dumps(params, indent=2)}")
        
        if args.dry_run:
            if not VERBOSE:
                print(f"Parsed parameters: {json.dumps(params, indent=2)}")

            print("Dry run mode: not executing requests.")
            return

        script_modules = []
        if args.script:
            for script_path in args.script:
                script_module = load_dynamic_script(script_path)
                if script_module:
                    print(f"Loaded dynamic script: {script_path}")
                    script_modules.append(script_module)
                else:
                    print(f"Failed to load dynamic script: {script_path}")
        params['script_modules'] = script_modules

        result = fire_requests(params, time_based=args.time_based)
        if VERBOSE:
            print(f"Results: {json.dumps(result, indent=2)}")

if __name__ == "__main__":
    main()
